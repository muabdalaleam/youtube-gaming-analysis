{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53861098-bd00-446e-a722-2bdffce10bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this python file we will update te videos data and channels data\n",
    "Usuing the id's that we have saved then save them as new parquet files\n",
    "just as the first steps in the note book but here we will make it more\n",
    "automated so we see the changes for videos and channels stats.\n",
    "\n",
    "and also we will need to not update the time-static data such as \n",
    "describtion or video_id they will be justin the base df.\n",
    "\n",
    "and we will use jupyter schduler to make it run daily on 8 am.\n",
    "\"\"\"\n",
    "\n",
    "# Importing packeges\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import googleapiclient.discovery\n",
    "from googleapiclient.discovery import build\n",
    "import googleapiclient.errors\n",
    "\n",
    "\n",
    "# Building youyube build\n",
    "API_KEY: str = \"AIzaSyANcOOmvv5fs6Gx7vKXucSelmScjx3V3Qg\"\n",
    "API_SERVICE_NAME = \"youtube\"\n",
    "API_VERSION = \"v3\"\n",
    "\n",
    "youtube = build(\n",
    "    API_SERVICE_NAME, API_VERSION, developerKey= API_KEY)\n",
    "\n",
    "\n",
    "print(\"starting ...\")\n",
    "\n",
    "# Collecting files into python variabels\n",
    "with open(\"../videos_ids.txt\", \"r\") as vid:\n",
    "    videos_ids: list = eval(vid.read())\n",
    "    \n",
    "with open(\"../channels_ids.txt\", \"r\") as vid:\n",
    "    channels_ids: list = eval(vid.read())\n",
    "    \n",
    "    \n",
    "# Extracting videos stats\n",
    "def get_video_stats(youtube, video_ids: list) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"This function takes the videos IDs list and request\n",
    "       for the statistics of the videos then save them into\n",
    "       a DataFrame.\"\"\"\n",
    "\n",
    "    all_video_info = []\n",
    "    thumbnails = []\n",
    "    videos_count = len(video_ids)\n",
    "\n",
    "    for i in range(0, videos_count, 50):\n",
    "        \n",
    "        chunk = video_ids[i:i+50]\n",
    "        processed_videos_count = i + len(chunk)\n",
    "        \n",
    "        # Giving the request for each 50 video in one time\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(chunk))\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Calculate the progress with updating it.\n",
    "        print(f\"Finished {processed_videos_count / videos_count * 100:.2f}% of loading the videos data\",\n",
    "              end= \"\\r\")\n",
    "        time.sleep(0.0001)\n",
    "\n",
    "        for video in response['items']:\n",
    "            video_json_encoder = {\"statistics\": ['viewCount', 'likeCount', 'commentCount']}\n",
    "\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for key in video_json_encoder.keys():\n",
    "                for val in video_json_encoder[key]:\n",
    "                    try:\n",
    "                        video_info[val] = video[key][val]\n",
    "                    except:\n",
    "                        video_info[val] = np.nan\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "\n",
    "    df = pd.DataFrame(all_video_info)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "videos_df = get_video_stats(youtube, videos_ids)\n",
    "    \n",
    "    \n",
    "# Getting channels data\n",
    "def get_channel_stats(youtube, channel_ids: list) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"This function returns the response of requested channels\n",
    "    into a pandas dataframe and saves it into JSON file\"\"\"\n",
    "\n",
    "    all_channels = []\n",
    "    chunk_size = 50\n",
    "\n",
    "    for l, i in enumerate(range(0, len(channel_ids), chunk_size)):\n",
    "\n",
    "        channel_ids_chunk = channel_ids[i:i + chunk_size]\n",
    "\n",
    "        request = youtube.channels().list(\n",
    "            part= \"snippet,contentDetails,statistics\",\n",
    "            id= \",\".join(channel_ids_chunk),\n",
    "            maxResults= 50)\n",
    "\n",
    "        response = request.execute()\n",
    "        with open(f'../responses/channel_response_{l + 1}.json', 'w') as f:\n",
    "            json.dump(response, f)\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "\n",
    "            data = {\"channel_name\": item[\"snippet\"][\"title\"],\n",
    "                    \"subscribers\": item[\"statistics\"][\"subscriberCount\"],\n",
    "                    \"total_views\": item[\"statistics\"][\"viewCount\"],\n",
    "                    \"video_count\": item[\"statistics\"][\"videoCount\"]}\n",
    "\n",
    "\n",
    "            all_channels.append(data)\n",
    "\n",
    "    return  pd.DataFrame(all_channels)\n",
    "\n",
    "channels_df = get_channel_stats(youtube, channels_ids)\n",
    "videos_df = videos_df.fillna(1)\n",
    "\n",
    "lines = [\n",
    "\"videos_df['likeCount'].astype(np.uint32)\",\n",
    "\"videos_df['viewCount'].astype(np.uint32)\",\n",
    "\"videos_df['commentCount'].astype(np.uint16)\"]\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    assination_line = line.split(\".\")[0] + \" = \" + line\n",
    "    exec(assination_line)\n",
    "\n",
    "\n",
    "channels_df[\"total_views\"] = channels_df[\"total_views\"].astype(np.uint64)\n",
    "channels_df[\"video_count\"] = channels_df[\"video_count\"].astype(np.uint16)\n",
    "channels_df[\"subscribers\"] = channels_df[\"subscribers\"].astype(np.uint32)\n",
    "\n",
    "\n",
    "# Saving updated data frames\n",
    "today = str(datetime.now().strftime('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "videos_df.to_parquet(f\"../data files/videos_{today.replace('-', '_')}.parquet\")\n",
    "channels_df.to_parquet(f\"../data files/channels_{today.replace('-', '_')}.parquet\")\n",
    "\n",
    "\n",
    "# Saving the stacked dataframes:\n",
    "\n",
    "channels_files = []\n",
    "videos_files = []\n",
    "\n",
    "directory_path = \"../data files/\"\n",
    "\n",
    "\n",
    "for file_name in os.listdir(directory_path):\n",
    "    \n",
    "    if \"channels\" in file_name and not(\"base\" in file_name):\n",
    "        channels_files.append(directory_path + file_name)\n",
    "        \n",
    "    elif (\"videos\" in file_name) and not(\"base\" in file_name):\n",
    "        videos_files.append(directory_path + file_name)\n",
    "     \n",
    "    \n",
    "\n",
    "videos_dfs = []\n",
    "channels_dfs = []\n",
    "\n",
    "for path in videos_files:\n",
    "    \n",
    "    video_df = pd.read_parquet(path)\n",
    "    start_sympol = path.index(\"2\")\n",
    "    end_sympol = path.index(\".p\")\n",
    "    \n",
    "    video_df[\"Collecting date\"] = path[start_sympol:end_sympol]\n",
    "    videos_dfs.append(video_df)\n",
    "\n",
    "for path in channels_files:\n",
    "    \n",
    "    channel_df = pd.read_parquet(path)\n",
    "    channel_df[\"Collecting date\"] = path[start_sympol +2:end_sympol+2]\n",
    "    channels_dfs.append(channel_df)\n",
    "    \n",
    "    \n",
    "conn = sqlite3.connect('../../database.db')\n",
    "    \n",
    "stacked_videos_df = pd.concat(videos_dfs, ignore_index=True)\n",
    "stacked_channels_df = pd.concat(channels_dfs, ignore_index=True)\n",
    "\n",
    "stacked_videos_df.to_sql('stacked_videos', conn, if_exists='replace', index=False)\n",
    "stacked_channels_df.to_sql('stacked_channels', conn, if_exists='replace', index=False)\n",
    "\n",
    "stacked_channels_df.to_pickle(\"../../Cleaned files/stacked_channels.pickle\")\n",
    "stacked_videos_df.to_pickle(\"../../Cleaned files/stacked_videos.pickle\")\n",
    "\n",
    "print(\"\\nDone ...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
