{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6318496-bd86-4156-9895-08f1d3dd4843",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center> **<span style=\"color: red\">NLP</span> steps and explnation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d6f47-3167-4401-9713-9bf09824a225",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we will use <span style=\"color: #ffd21e\">**Hugging face**</span> trasformers pre trained models becuase it'll take so much effort to label<br>\n",
    "all of our text columns \"<ins>Comments, Describtion, Title</ins>\" and also we will extract categorical and numirecal<br>\n",
    "features from the text columns.<br><br>\n",
    "\n",
    "##### **Steps in detail:**\n",
    "<ul>\n",
    "    <li>Extract categorical features from text columns sush as:<ol> \n",
    "        <li>Contains emojies\n",
    "        <li>Positive or negative\n",
    "        <li>Language\n",
    "        <li>Video stats range</ol>\n",
    "    <li> Doing analysis for most common words in comments and titles (Word cloud)\n",
    "    <li> Extracing the length of the comments, descriptions & titles\n",
    "    <li> One hot encoding\n",
    "    <li> Stemming the data and tokenizing it\n",
    "    <li> Saving those new data into new <strong>one</strong> Df to use in the final ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f4b08-de12-4220-8f8b-f7091dc4b3a8",
   "metadata": {},
   "source": [
    "##### **Important note:**\n",
    "The data in this analysis may be a little bit biased becuase all comments is made to be at least more than <br>\n",
    "$50$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb66a5d-927b-4f7e-9cb4-16c09f5fa91f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center> **Importing the <span style=\"color: red\">Packeges</span>**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5ee00c6-0b2a-49ca-b100-17a23aeeb885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import nltk\n",
    "import emoji\n",
    "import pickle\n",
    "import sqlite3\n",
    "import svgwrite\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import huggingface_hub\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from wordcloud import (WordCloud,\n",
    "                       STOPWORDS,\n",
    "                       ImageColorGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb01020a-db91-470e-a62c-c7ceeb297930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fbe94cc825416bbb424b703ab93146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FreeComp\\AppData\\Local\\Temp\\ipykernel_15264\\2025956571.py:5: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'svg')\n"
     ]
    }
   ],
   "source": [
    "with open(\"../Data analysis/functions/z-score.pickle\", \"rb\") as f:\n",
    "    z_score = pickle.load(f)\n",
    "\n",
    "huggingface_hub.notebook_login()\n",
    "set_matplotlib_formats('pdf', 'svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce01ab5-a514-4ed8-8bdc-d57b5e8bb339",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center> **Reading the <span style=\"color: red\">data</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb06258-74c7-43e6-b028-b047ec4bd479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_COLUMNS = [\"title\", \"comments\", \"description\", \"channelTitle\", \"about\"]\n",
    "\n",
    "con = sqlite3.connect('../database.db')\n",
    "\n",
    "df = pd.read_sql_query(\"\"\"   \n",
    "                        \n",
    "                        SELECT *\n",
    "                        FROM base_videos AS bv\n",
    "\n",
    "                        INNER JOIN base_channels  AS bc ON\n",
    "                            bc.channel_name = bv.channelTitle\n",
    "\n",
    "                        INNER JOIN comments  AS c ON\n",
    "                            c.video_id = bv.video_id\"\"\", con)\n",
    "\n",
    "con.close()\n",
    "\n",
    "df = df.T.drop_duplicates().T # dropping duplicated cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4882caaa-3d9d-4d46-927e-0e6465db004a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>definition</th>\n",
       "      <th>duration_in_minutes</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>total_views</th>\n",
       "      <th>date</th>\n",
       "      <th>video_count</th>\n",
       "      <th>about</th>\n",
       "      <th>country</th>\n",
       "      <th>comments</th>\n",
       "      <th>like_counts</th>\n",
       "      <th>reply_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>ROtSxPudwI4</td>\n",
       "      <td>Z1 Gaming</td>\n",
       "      <td>Finally Unlocking The AQUARIUM T2 !! Planet Cr...</td>\n",
       "      <td>#planetcrafter  #z1gaming\\n\\nA space survival ...</td>\n",
       "      <td>['planet crafter', 'the planet crafter', 'plan...</td>\n",
       "      <td>2023-03-28 12:30:23+00:00</td>\n",
       "      <td>20916</td>\n",
       "      <td>1390</td>\n",
       "      <td>65</td>\n",
       "      <td>hd</td>\n",
       "      <td>22.78125</td>\n",
       "      <td>512000</td>\n",
       "      <td>129586042</td>\n",
       "      <td>2016-03-23 00:00:00</td>\n",
       "      <td>3080</td>\n",
       "      <td>Hey everyone I'm Z and welcome to my channel, ...</td>\n",
       "      <td>US</td>\n",
       "      <td>In a regular progression you unlock fish farms...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>1RgnnTyP9rI</td>\n",
       "      <td>Grizzley Gang Gaming</td>\n",
       "      <td>Episode 25.3: The Entire City Got A Bag On My ...</td>\n",
       "      <td>TEE GRIZZLEY Playing GTA RP\\nEpisode 25.3: Of ...</td>\n",
       "      <td>['tee grizzley gta 5', 'tee grizzley', 'tee gr...</td>\n",
       "      <td>2023-03-23 01:00:10+00:00</td>\n",
       "      <td>128886</td>\n",
       "      <td>2546</td>\n",
       "      <td>203</td>\n",
       "      <td>hd</td>\n",
       "      <td>56.875</td>\n",
       "      <td>600000</td>\n",
       "      <td>187024461</td>\n",
       "      <td>2021-01-19 00:00:00</td>\n",
       "      <td>1306</td>\n",
       "      <td>TEE GRIZZLEY (GRIZZLEY WORLD OWNER):\\nhttps://...</td>\n",
       "      <td>US</td>\n",
       "      <td>ğŸ¤¦ğŸ½â€â™‚ï¸ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ PAYING TO EAT IT IS CRAZY AF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Lgbl1qI6Iak</td>\n",
       "      <td>Sumo Gaming</td>\n",
       "      <td>The *RANDOM* SKIN CHALLENGE In Garena Free Fire</td>\n",
       "      <td>SUB TO BE A LEGEND - https://bit.ly/3A2cBs9\\n\\...</td>\n",
       "      <td>['sumo gaming', 'garena free fire', 'free fire...</td>\n",
       "      <td>2022-03-29 05:00:08+00:00</td>\n",
       "      <td>2776845</td>\n",
       "      <td>68285</td>\n",
       "      <td>703</td>\n",
       "      <td>hd</td>\n",
       "      <td>10.03125</td>\n",
       "      <td>619000</td>\n",
       "      <td>65969205</td>\n",
       "      <td>2019-10-13 00:00:00</td>\n",
       "      <td>223</td>\n",
       "      <td>Hello, My name is Sumo.\\nThis channel make con...</td>\n",
       "      <td>IN</td>\n",
       "      <td>POTY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>70HDU3K1Fv8</td>\n",
       "      <td>Predator Gaming</td>\n",
       "      <td>It Lies Within | Episode 3: Level Up | Predato...</td>\n",
       "      <td>Who is friend or foe? Kya will need to figure ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2021-09-28 13:00:12+00:00</td>\n",
       "      <td>392172</td>\n",
       "      <td>1830</td>\n",
       "      <td>98</td>\n",
       "      <td>hd</td>\n",
       "      <td>4.898438</td>\n",
       "      <td>42100</td>\n",
       "      <td>5997924</td>\n",
       "      <td>2017-04-18 00:00:00</td>\n",
       "      <td>203</td>\n",
       "      <td>Want to see what we've been cooking? New gamin...</td>\n",
       "      <td>US</td>\n",
       "      <td>I would like to be a predator but... what a go...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>cvOHtIkawS8</td>\n",
       "      <td>Judo Sloth Gaming</td>\n",
       "      <td>New Super Miner Explained (Clash of Clans)</td>\n",
       "      <td>The December Update brings a New Troop, the Su...</td>\n",
       "      <td>['clash of clans', 'coc', 'clash of clans game...</td>\n",
       "      <td>2022-12-10 13:00:30+00:00</td>\n",
       "      <td>415656</td>\n",
       "      <td>19025</td>\n",
       "      <td>783</td>\n",
       "      <td>hd</td>\n",
       "      <td>8.046875</td>\n",
       "      <td>2400000</td>\n",
       "      <td>473754517</td>\n",
       "      <td>2014-02-04 00:00:00</td>\n",
       "      <td>1784</td>\n",
       "      <td>A Clash of Clans based channel focusing on hel...</td>\n",
       "      <td>GB</td>\n",
       "      <td>Subscribe for tomorrow Sneak Peek video, as we...</td>\n",
       "      <td>296</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id          channelTitle  \\\n",
       "2826  ROtSxPudwI4             Z1 Gaming   \n",
       "1839  1RgnnTyP9rI  Grizzley Gang Gaming   \n",
       "768   Lgbl1qI6Iak           Sumo Gaming   \n",
       "1590  70HDU3K1Fv8       Predator Gaming   \n",
       "935   cvOHtIkawS8     Judo Sloth Gaming   \n",
       "\n",
       "                                                  title  \\\n",
       "2826  Finally Unlocking The AQUARIUM T2 !! Planet Cr...   \n",
       "1839  Episode 25.3: The Entire City Got A Bag On My ...   \n",
       "768     The *RANDOM* SKIN CHALLENGE In Garena Free Fire   \n",
       "1590  It Lies Within | Episode 3: Level Up | Predato...   \n",
       "935          New Super Miner Explained (Clash of Clans)   \n",
       "\n",
       "                                            description  \\\n",
       "2826  #planetcrafter  #z1gaming\\n\\nA space survival ...   \n",
       "1839  TEE GRIZZLEY Playing GTA RP\\nEpisode 25.3: Of ...   \n",
       "768   SUB TO BE A LEGEND - https://bit.ly/3A2cBs9\\n\\...   \n",
       "1590  Who is friend or foe? Kya will need to figure ...   \n",
       "935   The December Update brings a New Troop, the Su...   \n",
       "\n",
       "                                                   tags  \\\n",
       "2826  ['planet crafter', 'the planet crafter', 'plan...   \n",
       "1839  ['tee grizzley gta 5', 'tee grizzley', 'tee gr...   \n",
       "768   ['sumo gaming', 'garena free fire', 'free fire...   \n",
       "1590                                                 []   \n",
       "935   ['clash of clans', 'coc', 'clash of clans game...   \n",
       "\n",
       "                    publishedAt viewCount likeCount commentCount definition  \\\n",
       "2826  2023-03-28 12:30:23+00:00     20916      1390           65         hd   \n",
       "1839  2023-03-23 01:00:10+00:00    128886      2546          203         hd   \n",
       "768   2022-03-29 05:00:08+00:00   2776845     68285          703         hd   \n",
       "1590  2021-09-28 13:00:12+00:00    392172      1830           98         hd   \n",
       "935   2022-12-10 13:00:30+00:00    415656     19025          783         hd   \n",
       "\n",
       "     duration_in_minutes subscribers total_views                 date  \\\n",
       "2826            22.78125      512000   129586042  2016-03-23 00:00:00   \n",
       "1839              56.875      600000   187024461  2021-01-19 00:00:00   \n",
       "768             10.03125      619000    65969205  2019-10-13 00:00:00   \n",
       "1590            4.898438       42100     5997924  2017-04-18 00:00:00   \n",
       "935             8.046875     2400000   473754517  2014-02-04 00:00:00   \n",
       "\n",
       "     video_count                                              about country  \\\n",
       "2826        3080  Hey everyone I'm Z and welcome to my channel, ...      US   \n",
       "1839        1306  TEE GRIZZLEY (GRIZZLEY WORLD OWNER):\\nhttps://...      US   \n",
       "768          223  Hello, My name is Sumo.\\nThis channel make con...      IN   \n",
       "1590         203  Want to see what we've been cooking? New gamin...      US   \n",
       "935         1784  A Clash of Clans based channel focusing on hel...      GB   \n",
       "\n",
       "                                               comments like_counts  \\\n",
       "2826  In a regular progression you unlock fish farms...           0   \n",
       "1839            ğŸ¤¦ğŸ½â€â™‚ï¸ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ PAYING TO EAT IT IS CRAZY AF           0   \n",
       "768                                                POTY           0   \n",
       "1590  I would like to be a predator but... what a go...           0   \n",
       "935   Subscribe for tomorrow Sneak Peek video, as we...         296   \n",
       "\n",
       "     reply_counts  \n",
       "2826            0  \n",
       "1839            0  \n",
       "768             0  \n",
       "1590            0  \n",
       "935            20  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d71c2d1-bed5-440e-a7c8-4dd52f37eb00",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  <center> **Extract <span style=\"color: red\">categorical</span> features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d86d3-1556-44fc-b2e8-4b85f74ab79f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Extract emojies count per text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d5ae23-d9c9-4307-811a-366b6cadfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_emojis_counts: list = []\n",
    "title_emojis_counts: list = []\n",
    "desc_emojis_counts: list = []\n",
    "\n",
    "for comment, title, desc in zip(df[\"comments\"], df[\"title\"], df[\"description\"]):\n",
    "    \n",
    "    comments_emojis_count: int = 0\n",
    "    title_emojis_count: int = 0\n",
    "    desc_emojis_count: int = 0\n",
    "    \n",
    "    for comment_char, title_char, desc_char in zip(comment, title, desc):\n",
    "        \n",
    "        if emoji.is_emoji(comment_char):\n",
    "            comments_emojis_count += 1\n",
    "            \n",
    "        if emoji.is_emoji(title_char):\n",
    "            title_emojis_count += 1\n",
    "            \n",
    "        if emoji.is_emoji(desc_char):\n",
    "            desc_emojis_count += 1\n",
    "    \n",
    "    comments_emojis_counts.append(comments_emojis_count)\n",
    "    title_emojis_counts.append(title_emojis_count)\n",
    "    desc_emojis_counts.append(desc_emojis_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba335f72-df88-4bc3-ace7-27a991b83a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"comments_emojis_count\"] = comments_emojis_counts\n",
    "df[\"title_emojis_count\"] = title_emojis_counts\n",
    "df[\"desc_emojis_count\"] = desc_emojis_counts\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d487f5b-5b6f-4b93-8fd3-0079446e4a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Language detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3385b1-79ec-42f1-9a2b-de5b3801a91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "language_detector = transformers.pipeline(\"text-classification\",\n",
    "                                          model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "                                          use_auth_token= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcab35d-9a81-487f-87dd-962631a5109b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "print(language_detector(\"Ù‡Ø°Ø§ Ø£Ø·ÙˆÙ„ Ù†Øµ Ù‚Ø¯ ØªÙ‚Ø±Ø£Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ Ø¥Ù†Ù‡ Ø·ÙˆÙŠÙ„ Ù„Ù„ØºØ§ÙŠØ©\", max_length=128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e40b4-01c6-478f-8672-219a5cb73e0a",
   "metadata": {},
   "source": [
    "Looks like we will take a lot to <ins>just classify the language</ins> of each text at least: $30\\times100$ second whitch equalls <span style=\"color: red\">**50 minutes.**</span><br>\n",
    "So we can't do translation for now becuase it at least may take **5 hours** to run so we will just use this language detetion and<br>\n",
    "optimize it by just sellecting one text column to detect which will be **Description column** and use the detected languages to<br>\n",
    "pridect video and comments language.<br><br>\n",
    "But we will detect the language using the **country of this video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48b629-95cc-437f-8bfc-87095cfd1382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e744162-5f9c-42e1-b6c2-09fd07d179d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_languages = {\n",
    "    'DE': 'German',\n",
    "    'US': 'English', 'PL': 'Polish',\n",
    "    'SA': 'Arabic', 'NP': 'Nepali',\n",
    "    'CA': 'English', 'ES': 'Spanish',\n",
    "    'TR': 'Turkish', 'IN': 'Hindi',\n",
    "    'EG': 'Arabic', 'GB': 'English',\n",
    "    'MX': 'Spanish', 'BR': 'Portuguese',\n",
    "    'PK': 'Urdu', 'FR': 'French',\n",
    "    'VN': 'Vietnamese', 'ID': 'Indonesian',\n",
    "    'AU': 'English', 'HU': 'Hungarian',\n",
    "    'NL': 'Dutch', 'BG': 'Bulgarian',\n",
    "    'JP': 'Japanese', 'SG': 'English', \n",
    "    'TH': 'Thai', 'PH': 'Tagalog',\n",
    "    'MT': 'Maltese', 'PE': 'Spanish',\n",
    "    'SE': 'Swedish', 'IT': 'Italian',\n",
    "    'KR': 'Korean', 'TW': 'Chinese',\n",
    "    'FI': 'Finnish', 'DZ': 'Arabic',\n",
    "    'BD': 'Bengali', 'AR': 'Spanish'}\n",
    "\n",
    "df[\"language\"] = df[\"country\"].replace(country_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f846902-adaa-4678-b296-f49c5459e4a4",
   "metadata": {},
   "source": [
    "This way may not be so accurate in detecting languages becuase there are some **indian videos** specifically with english and<br>\n",
    "because some commetns come in deffrint languages but at least this way is more accurate than `roberta-language-detection`<br>\n",
    "model because this model sometimes come with wierd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090791f-b041-44a6-90a6-75f463aec37b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Sentiment analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840cb75c-2375-45d3-b354-fdfb69ec45ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_classifier = transformers.pipeline(model= \"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "print(sentiment_classifier(\"Hello here in my analysis, Have a nice day !!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc8fbc-0843-4077-a6b4-d6c83abc7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sentiment_classifier(text: str) -> np.int8:\n",
    "    \"\"\"This code takes a text and return you if it's positive\n",
    "        or negative as 1 for positive, 0 for natural -1 for negative\n",
    "        and -10 for unclassified.\n",
    "    \n",
    "    @params: a string text\n",
    "    @return: 1, 0 or -1 in np.int8 dtype\"\"\"\n",
    "    \n",
    "    sentiment_type_encoder: dict = {\"POS\": 1,\n",
    "                                    \"NEG\": -1,\n",
    "                                    \"NEU\": 0,\n",
    "                                    \"unclassified\": -10}\n",
    "    \n",
    "    try:\n",
    "        sentiment_type: str = sentiment_classifier(text)[0][\"label\"]\n",
    "        \n",
    "    except:\n",
    "        sentiment_type: str = \"unclassified\"\n",
    "        \n",
    "    return  np.int8(sentiment_type_encoder[sentiment_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8916528-4171-4ede-9e99-ff4b3b9ae9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"sentiments\"] = df[\"Comments\"][:5].apply(\n",
    "    lambda x: my_sentiment_classifier(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056e4ad-fc7e-42bc-9bb1-dca16fed8e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"sentiments\", \"Comments\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4b794-24d8-4c52-8ef9-7c2078fca8eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Video stats range*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826ebf3-aea5-4381-9097-814d02cad0fa",
   "metadata": {},
   "source": [
    "We will classify each column of **[Comments, Likes, Views]** into 6 groups manually like:\n",
    "<ul>\n",
    "    <li>  $1\\sim3,000$ Views\n",
    "    <li>  $3,000\\sim10,000$ Views\n",
    "    <li>  $10,000\\sim50,000$ Views\n",
    "    <li>  $50,000\\sim100,000$ Views\n",
    "    <li>  $100,000\\sim300,000$ Views\n",
    "    <li>  $300,000\\sim\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5313b-6d7d-4713-bbeb-46b461daceb7",
   "metadata": {},
   "source": [
    "We will see the max calues of each column so we can optimize it with the best dtype for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa25cea-a4d4-4e1f-b5fe-50bf1cc9e6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\" The max value of view count column is: {df['viewCount'].astype(int).max()}\\n\",\n",
    "      f\"The max value of like count column ias: {df['likeCount'].astype(int).max()}\\n\",\n",
    "      f\"The max value of comment count column is: {df['commentCount'].astype(int).max()}\\n\",\n",
    "      f\"The max value of subscribers column is: {df['subscribers'].astype(int).max()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e1b26-9182-4b33-8c07-dadafbdb70cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.astype({\"commentCount\": np.uint16, \"viewCount\": np.uint32,\n",
    "                \"likeCount\": np.uint16, \"subscribers\": np.uint32})\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd3f47-9eae-4d3d-a60d-f7a74b3ff44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"cat_view_count\"] = pd.cut(df['viewCount'],\n",
    "                         bins=[1, 3_000, 10_000, 50_000, 100_000, 300_000, 999_999_999_999],\n",
    "                         labels=[\"from 1 to 3,000\", \"from 3,000 to 10,000\",\n",
    "                                 \"from 10,000 to 50,000\", \"from 50,000 to 100,000\",\n",
    "                                 \"from 100,000 to 300,000\", \"more than 300,000\"])\n",
    "\n",
    "df[\"cat_comment_count\"] = pd.cut(df['commentCount'],\n",
    "                         bins=[50, 100, 150, 200, 400, 600, 999_999_999_999],\n",
    "                         labels=[\"from 50 to 100\", \"from 100 to 150\",\n",
    "                                 \"from 150 to 200\", \"from 200 to 400\",\n",
    "                                 \"from 400 to 600\", \"more than 600\"])\n",
    "\n",
    "df[\"cat_like_count\"] = pd.cut(df['likeCount'],\n",
    "                         bins=[0, 1_000, 5_000, 10_000, 50_000, 150_000, 999_999_999_999],\n",
    "                         labels=[\"from 1 to 1,000\", \"from 1,000 to 5,000\",\n",
    "                                 \"from 5,000 to 10,000\", \"from 10,000 to 50,000\",\n",
    "                                 \"from 50,000 to 150,000\", \"more than 150,000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15043a-4b10-4ba3-a74b-c05fa3b15fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"cat_view_count\", \"cat_comment_count\", \"cat_like_count\",\n",
    "    \"viewCount\", \"commentCount\", \"likeCount\"]].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f83d0d-3080-4ce4-95a9-2035bad6634a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Text columns length*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f55aa-01c0-431b-9451-cfc1b4515e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in TEXT_COLUMNS:\n",
    "    df[f\"{col}_length\"] = df[col].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81007d35-99d7-4b33-b066-e58fda430082",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong>Text visualiztion with<span style = \"color: red\"> WordCloud</span></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11cf8d-2669-4851-85ab-51e7bdfb3827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coloring = np.array(Image.open(\"../imgs/youtube_gaming_logo.png\"))\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "comments = ''.join(df[\"comments\"][df[\"language\"] == \"English\"].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ea87b-5990-47ee-9840-358f2ca2b06c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments_wc = WordCloud(background_color= \"white\", max_words= 300, mask= coloring,\n",
    "               stopwords= stopwords, max_font_size= 80, random_state= 42,\n",
    "               font_path= '../Data analysis/assets/fonts/FranklinGothic.ttf',\n",
    "               collocations=False)\n",
    "\n",
    "image_colors = ImageColorGenerator(np.array(coloring))\n",
    "comments_wc.generate(comments)\n",
    "comments_wc = comments_wc.recolor(color_func= image_colors)\n",
    "\n",
    "comments_svg = comments_wc.to_svg(embed_font=True)\n",
    "\n",
    "# Save the SVG code to a file\n",
    "with open(\"../plots/comments_word_cloud.svg\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(comments_svg)\n",
    "    \n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.imshow(comments_wc, interpolation=\"bilinear\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.title(\"Comments word cloud\")\n",
    "fig.set_size_inches(10, 8)\n",
    "plt.savefig(\"../plots/comments_word_cloud.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d205eae-0082-403d-85a3-e7eff934769b",
   "metadata": {},
   "source": [
    "Now we will make a word cloud for **video titles data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf85c9-fa11-4bf6-af23-18962eae6313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "popular_emojis = [\"ğŸ˜Š\", \"ğŸ˜\", \"ğŸ˜˜\", \"ğŸ˜œ\", \"ğŸ˜\", \"ğŸ˜‚\", \"ğŸ˜­\", \"ğŸ˜¡\", \"ğŸ˜ \", \"ğŸ˜©\", \"ğŸ˜«\", \"ğŸ˜\", \"ğŸ˜Ÿ\", \"ğŸ˜¢\", \"ğŸ˜¥\", \"ğŸ˜°\", \"ğŸ˜±\",\n",
    "                       \"ğŸ˜³\", \"ğŸ˜·\", \"ğŸ‘\", \"ğŸ‘\", \"ğŸ‘Œ\", \"ğŸ‘\", \"ğŸ™Œ\", \"ğŸ‘‹\", \"ğŸ’ª\", \"ğŸ™\", \"â¤ï¸\", \"ğŸ’”\", \"ğŸ’•\", \"ğŸ’–\", \"ğŸ’˜\", \"ğŸ’™\", \"ğŸ’š\",\n",
    "                       \"ğŸ’›\", \"ğŸ’œ\", \"ğŸ’¯\", \"ğŸ”¥\", \"ğŸŒŸ\", \"âœ¨\", \"â­\", \"ğŸŒˆ\", \"ğŸŒº\", \"ğŸ•\", \"ğŸ”\", \"ğŸŸ\", \"ğŸ¦\", \"ğŸ­\", \"ğŸ©\", \"ğŸª\", \"ğŸº\",\n",
    "                       \"ğŸ»\", \"ğŸ·\", \"ğŸ¸\", \"ğŸ‚\", \"ğŸ\", \"ğŸ‰\", \"ğŸŠ\", \"ğŸˆ\", \"ğŸµ\", \"ğŸ¶\", \"ğŸ¼\", \"ğŸ§\", \"ğŸ¤\", \"ğŸ¸\", \"ğŸ¹\", \"ğŸº\", \"ğŸ·\",\n",
    "                       \"ğŸ»\", \"ğŸ¬\", \"ğŸ¥\", \"ğŸ¦\", \"ğŸ“·\", \"ğŸ“¹\", \"ğŸ“º\", \"ğŸ“»\", \"ğŸ’»\", \"ğŸ“±\", \"ğŸ’¡\", \"ğŸ”‘\", \"ğŸ”¨\", \"ğŸ”¥\", \"ğŸ’°\", \"ğŸ’³\", \"ğŸ’¼\",\n",
    "                       \"ğŸ“…\", \"ğŸ“†\", \"ğŸ“ˆ\", \"ğŸ“‰\", \"ğŸ“Š\", \"ğŸ“‹\", \"ğŸ“\", \"ğŸ“\", \"ğŸ“\", \"ğŸ”’\", \"ğŸ”“\", \"ğŸ”\", \"ğŸ”\", \"ğŸš€\", \"ğŸš‘\", \"ğŸš’\", \"ğŸš“\",\n",
    "                       \"ğŸš•\", \"ğŸš—\", \"ğŸš™\", \"ğŸšš\", \"ğŸš¢\", \"ğŸš¤\", \"ğŸš²\", \"ğŸš¶\", \"ğŸš¶â€â™€ï¸\", \"ğŸƒ\", \"ğŸƒâ€â™€ï¸\", \"âš½\", \"ğŸ€\", \"ğŸˆ\", \"ğŸ¾\", \"ğŸ\", \"ğŸ‰\",\n",
    "                       \"ğŸ±\", \"ğŸ“\", \"ğŸ¸\", \"ğŸ¥Š\", \"ğŸ¥‹\", \"ğŸ®\", \"ğŸ•¹ï¸\", \"ğŸ²\", \"ğŸƒ\"]\n",
    "\n",
    "\n",
    "for emoji in popular_emojis:\n",
    "    stopwords.add(emoji)\n",
    "    \n",
    "titels = ''.join(df[\"title\"][df[\"language\"] == \"English\"].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef628a-ad55-4443-b30b-d4ba0a0c777e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coloring = np.array(Image.open(\"../imgs/joystick.png\"))\n",
    "\n",
    "titels_wc = WordCloud(background_color= \"white\", max_words= 2000, mask= coloring,\n",
    "               stopwords= stopwords, max_font_size= 80, random_state= 42,\n",
    "               font_path= '../Data analysis/assets/fonts/FranklinGothic.ttf')\n",
    "\n",
    "titels_wc.generate(titels)\n",
    "image_colors = ImageColorGenerator(np.array(coloring))\n",
    "titels_wc = titels_wc.recolor(color_func= image_colors)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.imshow(titels_wc, interpolation= \"bilinear\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "titels_svg = titels_wc.to_svg(embed_font=True)\n",
    "\n",
    "with open(\"../plots/videos_titels_word_cloud.svg\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(titels_svg)\n",
    "\n",
    "plt.title(\"Video titles word cloud\")\n",
    "fig.set_size_inches(10, 8)\n",
    "plt.savefig(\"../plots/videos_titels_word_cloud.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5551db7-f409-4c22-9a63-60b297f2f104",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style = \"color: red\">NLP</span> preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30e29d-2854-4429-bfc8-e20d6b7681ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Removing stop words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99fa01-d3b2-4484-bc19-6b53499403c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download()\n",
    "\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('wordnet', force= True)\n",
    "nltk.download('stopwords', force=True)\n",
    "nltk.download('stopwords-hi', force=True)\n",
    "nltk.download('stopwords-ar', force=True)\n",
    "nltk.download('averaged_perceptron_tagger', force=True)\n",
    "\n",
    "\n",
    "en_stopwords = set(stopwords.words('english')) \n",
    "ar_stopwords = set(stopwords.words('arabic')) \n",
    "# hi_stopwords = set(stopwords.words('hindi')) \n",
    "\n",
    "all_stopwords = en_stopwords.union(ar_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43419a-e185-45c0-8a2a-6a308eca0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_dropper(words: list, stopwords: set) -> list:\n",
    "    \n",
    "    # Removing stop words from unalphabetical chars\n",
    "    filtered_words = [re.sub(r\"[\\W_]\", \"\", word) for word in words\n",
    "                      if not word in stopwords]\n",
    "    \n",
    "    filtered_words = list(filter(lambda item: item is not \"\", filtered_words))\n",
    "    return  filtered_words\n",
    "\n",
    "\n",
    "for col in TEXT_COLUMNS:\n",
    "    df[f\"{col}_tokens\"] = df[col].apply(lambda text: nltk.word_tokenize(text.lower()))\n",
    "    df[f\"{col}_tokens\"] = df[f\"{col}_tokens\"].apply(lambda text: stopwords_dropper(text,\n",
    "                                                                     all_stopwords))\n",
    "\n",
    "df[\"comments_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153b801-9d49-457e-b771-8d37fe7d982c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Part-of-speech (POS) Tagging*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaeeb6-f0cc-4ddb-9a50-6b6d54169268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in TEXT_COLUMNS:\n",
    "    df[f\"{col}_pos_tags\"] = df[f\"{col}_tokens\"].apply(lambda words: nltk.pos_tag(words))\n",
    "\n",
    "df[\"about_pos_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147edc4-d4a1-48c1-9fa7-5ed1242ecec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Lemmatization and dropping duplicated words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64504cf0-014a-42ab-aa9b-730cfd69a511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag: str) -> str:\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cc152-86d7-4df7-901d-2f223e6e0187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "lemmatized_words_group = []\n",
    "\n",
    "for col in TEXT_COLUMNS:\n",
    "    for index, row in df.iterrows():\n",
    "        for token, pos_tag in zip(row[f\"{col}_tokens\"], row[f\"{col}_pos_tags\"]):\n",
    "\n",
    "            wordnet_pos = get_wordnet_pos(pos_tag[1])\n",
    "            lemmatized_words_group.append(lemmatizer.lemmatize(token, pos= wordnet_pos))\n",
    "            lemmatized_words_group = list(set(lemmatized_words_group)) # Dropping duplicates\n",
    "\n",
    "\n",
    "        lemmatized_words.append(lemmatized_words_group)\n",
    "        lemmatized_words_group = [] # clearing this list\n",
    "    \n",
    "    df[f\"{col}_tokens\"] = lemmatized_words\n",
    "    lemmatized_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debe01c-effc-44ff-b48b-f7907907ce8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"channelTitle_tokens\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fe229-ec6c-41f1-ab24-a2eef29cd9dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *One hot encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7470c3-dfeb-429f-bdf3-7d124daf1b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"language\"] = df[\"language\"].astype(\"category\").cat.codes\n",
    "df[\"definition\"] = df[\"definition\"].astype(\"category\").cat.codes\n",
    "# df[\"sentiments\"] = df[\"sentiments\"].astype(\"category\")\n",
    "\n",
    "df[\"cat_view_count\"] = df[\"cat_view_count\"].replace({\"from 1 to 3,000\": 1, \"from 3,000 to 10,000\": 2,\n",
    "                                                     \"from 10,000 to 50,000\": 3, \"from 50,000 to 100,000\": 4,\n",
    "                                                     \"from 100,000 to 300,000\": 5, \"more than 300,000\": 6})\n",
    "\n",
    "df[\"cat_like_count\"] = df[\"cat_like_count\"].replace({\"from 1 to 1,000\": 1, \"from 1,000 to 5,000\": 2,\n",
    "                                                     \"from 5,000 to 10,000\": 3, \"from 10,000 to 50,000\": 4,\n",
    "                                                     \"from 50,000 to 150,000\": 5, \"more than 150,000\": 6})\n",
    "\n",
    "df[\"cat_comment_count\"] = df[\"cat_comment_count\"].replace({\"from 50 to 100\": 1, \"from 100 to 150\": 2,\n",
    "                                                           \"from 150 to 200\": 3, \"from 200 to 400\": 4,\n",
    "                                                           \"from 400 to 600\": 5, \"more than 600\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df1754-3787-4ad3-bdf3-d64d6f3abb7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"language\", \"definition\", \"cat_view_count\",\n",
    "    \"cat_like_count\", \"cat_comment_count\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb03b0-4af0-49eb-9ca4-48785a22f5dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style = \"color: red\">Text</span> classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f54fed9-fbd9-4268-ace1-f300f8f02c68",
   "metadata": {},
   "source": [
    "#### *Preparing training & testing data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05670e1-d0a0-4e77-83d9-126ce045b109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[[\"channelTitle_tokens\", \"comments_tokens\"]].astype(str)\n",
    "\n",
    "\n",
    "y = df[[\"cat_view_count\", \"cat_like_count\"]]\n",
    "\n",
    "# # Tokenize the text feature.\n",
    "# text_tokens = df['comments_tokens'].apply(lambda x: ' '.join(x).split())\n",
    "\n",
    "# Tokenize the other feature.\n",
    "# other_tokens = df['other_feature'].apply(lambda x: x.split())\n",
    "\n",
    "# idx = np.random.randint(4_000, size= 5)\n",
    "# pd.DataFrame(X[idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541229c-4843-4bd6-b623-66fed630ebb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/ 5, random_state= 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44551aa-b17a-4da9-a39c-adcb0c7b3272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd48fa5-cfa1-4fa0-b0ae-fb303ed5356a",
   "metadata": {},
   "source": [
    "#### *XGBoost classfier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f94e8-7ab4-440d-b746-7b60d50f8882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# Fit the model and make predictions\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc59898-afbe-44ef-a0ef-f5304a88a241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
