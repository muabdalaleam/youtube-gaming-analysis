{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6318496-bd86-4156-9895-08f1d3dd4843",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center> **<span style=\"color: red\">NLP</span> steps and explnation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d6f47-3167-4401-9713-9bf09824a225",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we will use <span style=\"color: #ffd21e\">**Hugging face**</span> trasformers pre trained models becuase it'll take so much effort to label<br>\n",
    "all of our text columns \"<ins>Comments, Describtion, Title</ins>\" and also we will extract categorical and numirecal<br>\n",
    "features from the text columns.<br><br>\n",
    "\n",
    "##### **Steps in detail:**\n",
    "<ul>\n",
    "    <li>Extract categorical features from text columns sush as:<ol> \n",
    "        <li>Contains emojies\n",
    "        <li>Positive or negative\n",
    "        <li>Language\n",
    "        <li>Video stats range</ol>\n",
    "    <li> Doing analysis for most common words in comments and titles (Word cloud)\n",
    "    <li> Extracing the length of the comments, descriptions & titles\n",
    "    <li> One hot encoding\n",
    "    <li> Stemming the data and tokenizing it\n",
    "    <li> Saving those new data into new <strong>one</strong> Df to use in the final ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f4b08-de12-4220-8f8b-f7091dc4b3a8",
   "metadata": {},
   "source": [
    "##### **Important note:**\n",
    "The data in this analysis may be a little bit biased becuase all comments is made to be at least more than <br>\n",
    "$50$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb66a5d-927b-4f7e-9cb4-16c09f5fa91f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center> **Importing the <span style=\"color: red\">Packeges</span>**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e5ee00c6-0b2a-49ca-b100-17a23aeeb885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import ast\n",
    "import time\n",
    "import nltk\n",
    "import emoji\n",
    "import pickle\n",
    "import sqlite3\n",
    "import svgwrite\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import huggingface_hub\n",
    "from nltk.stem import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from wordcloud import (WordCloud,\n",
    "                       STOPWORDS,\n",
    "                       ImageColorGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb01020a-db91-470e-a62c-c7ceeb297930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c286b18d0f845ce813e39722e562d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FreeComp\\AppData\\Local\\Temp\\ipykernel_8952\\2025956571.py:5: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('pdf', 'svg')\n"
     ]
    }
   ],
   "source": [
    "with open(\"../Data analysis/functions/z-score.pickle\", \"rb\") as f:\n",
    "    z_score = pickle.load(f)\n",
    "\n",
    "huggingface_hub.notebook_login()\n",
    "set_matplotlib_formats('pdf', 'svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce01ab5-a514-4ed8-8bdc-d57b5e8bb339",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center> **Reading the <span style=\"color: red\">data</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb06258-74c7-43e6-b028-b047ec4bd479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_COLUMNS = [\"title\", \"comments\", \"description\", \"channelTitle\", \"about\"]\n",
    "\n",
    "con = sqlite3.connect('../database.db')\n",
    "\n",
    "df = pd.read_sql_query(\"\"\"   \n",
    "                        \n",
    "                        SELECT *\n",
    "                        FROM base_videos AS bv\n",
    "\n",
    "                        INNER JOIN base_channels  AS bc ON\n",
    "                            bc.channel_name = bv.channelTitle\n",
    "\n",
    "                        INNER JOIN comments  AS c ON\n",
    "                            c.video_id = bv.video_id\"\"\", con)\n",
    "\n",
    "con.close()\n",
    "\n",
    "df = df.T.drop_duplicates().T # dropping duplicated cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4882caaa-3d9d-4d46-927e-0e6465db004a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>definition</th>\n",
       "      <th>duration_in_minutes</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>total_views</th>\n",
       "      <th>date</th>\n",
       "      <th>video_count</th>\n",
       "      <th>about</th>\n",
       "      <th>country</th>\n",
       "      <th>comments</th>\n",
       "      <th>like_counts</th>\n",
       "      <th>reply_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>A7luS6IioBk</td>\n",
       "      <td>Roplex Gaming</td>\n",
       "      <td>Trading MEGA NAGA DRAGON (HUGE OVER PAY ADOPT ME)</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>['roblox', 'roplex gaming', 'adopt me', 'roblo...</td>\n",
       "      <td>2023-03-03 21:06:32+00:00</td>\n",
       "      <td>61007</td>\n",
       "      <td>1178</td>\n",
       "      <td>281</td>\n",
       "      <td>hd</td>\n",
       "      <td>9.75</td>\n",
       "      <td>650000</td>\n",
       "      <td>189972458</td>\n",
       "      <td>2019-05-11 00:00:00</td>\n",
       "      <td>929</td>\n",
       "      <td>I like to do adopt me trading videos on roblox...</td>\n",
       "      <td>US</td>\n",
       "      <td>slayyyyyyyy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>vvnjeMtrZvY</td>\n",
       "      <td>Yousef Gaming</td>\n",
       "      <td>12 ŸÇŸÅŸÑ #6 | ÿ¨ÿ≤ÿ° ÿ¨ÿØŸäÿØ ÿÆÿ±ÿßŸÅŸä üòªüî• !!</td>\n",
       "      <td>ŸÑÿß ÿ™ÿ¥ÿ∫ŸÑŸÉ ÿßŸÑŸÖŸÇÿßÿ∑ÿπ ÿπŸÜ ÿßŸÑÿµŸÑŸàÿßÿ™ üíñ\\nÿµŸÑŸàÿß ÿπŸÑŸâ ŸÖÿ≠ŸÖÿØ Ô∑∫...</td>\n",
       "      <td>['ŸäŸàÿ≥ŸÅ ÿßÿ≠ŸÖÿØ', 'ŸäŸàÿ≥ŸÅ', 'ÿßÿ≠ŸÖÿØ', 'ŸäŸàÿ≥ŸÅ ÿ¨ŸäŸÖŸÜÿ¨', 'y...</td>\n",
       "      <td>2022-07-16 14:00:53+00:00</td>\n",
       "      <td>56921</td>\n",
       "      <td>4607</td>\n",
       "      <td>177</td>\n",
       "      <td>hd</td>\n",
       "      <td>12.101562</td>\n",
       "      <td>1790000</td>\n",
       "      <td>95599249</td>\n",
       "      <td>2016-11-05 00:00:00</td>\n",
       "      <td>270</td>\n",
       "      <td>ÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ ÿ≠ÿ®ÿßŸäÿ®Ÿä ŸÅŸä ŸÇŸÜÿßÿ© ÿßŸÑÿ£ŸÑÿπÿßÿ®\\nŸáŸÜÿß ŸÅŸä Yous...</td>\n",
       "      <td>SA</td>\n",
       "      <td>‚ô°</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>o3D9g0ohRIo</td>\n",
       "      <td>Gaming Insects</td>\n",
       "      <td>üòÇI Became A Giant Alex To Fooled This Girl in ...</td>\n",
       "      <td>üòÇI Became A Giant Alex To Fooled This Girl in ...</td>\n",
       "      <td>['Trolling My New Girlfriend as GIANT ALEX in ...</td>\n",
       "      <td>2023-03-02 07:30:08+00:00</td>\n",
       "      <td>144703</td>\n",
       "      <td>5977</td>\n",
       "      <td>628</td>\n",
       "      <td>hd</td>\n",
       "      <td>20.1875</td>\n",
       "      <td>694000</td>\n",
       "      <td>79491185</td>\n",
       "      <td>2019-08-22 00:00:00</td>\n",
       "      <td>415</td>\n",
       "      <td>Guys Im Trying My Best Ki App Logo Ko Bohot En...</td>\n",
       "      <td>IN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553</th>\n",
       "      <td>uWpup9oq54w</td>\n",
       "      <td>Stealth17 Gaming</td>\n",
       "      <td>The German Fleet Arrives - Ultimate Admiral Dr...</td>\n",
       "      <td>The German fleet suddenly showed up in the Asi...</td>\n",
       "      <td>['ultimate admiral dreadnoughts', 'ultimate ad...</td>\n",
       "      <td>2023-02-20 16:00:36+00:00</td>\n",
       "      <td>13260</td>\n",
       "      <td>691</td>\n",
       "      <td>55</td>\n",
       "      <td>hd</td>\n",
       "      <td>45.15625</td>\n",
       "      <td>102000</td>\n",
       "      <td>41969216</td>\n",
       "      <td>2014-08-18 00:00:00</td>\n",
       "      <td>3844</td>\n",
       "      <td>Calm &amp; collected commentaries on strategy game...</td>\n",
       "      <td>NL</td>\n",
       "      <td>Wow, that was one big slaughter !!! I can't aw...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>-iojQuqlWkU</td>\n",
       "      <td>DK Gaming</td>\n",
       "      <td>ƒ∞LK √áOCUKLAR ! (THE SIMS 4 100 BEBEK CHALLENGE...</td>\n",
       "      <td>‚úÖ DK Gaming ten selamlar ben Duygu K√∂seoƒülu bu...</td>\n",
       "      <td>['dkgaming', 'duygu', 'duygu k√∂seoƒülu', '√ºcret...</td>\n",
       "      <td>2022-12-05 12:00:11+00:00</td>\n",
       "      <td>30649</td>\n",
       "      <td>1335</td>\n",
       "      <td>189</td>\n",
       "      <td>hd</td>\n",
       "      <td>40.3125</td>\n",
       "      <td>252000</td>\n",
       "      <td>54565589</td>\n",
       "      <td>2019-03-07 00:00:00</td>\n",
       "      <td>515</td>\n",
       "      <td>Daha fazla oyun daha fazla eƒülence! Duygu K√∂se...</td>\n",
       "      <td>TR</td>\n",
       "      <td>1 orada age up var neden yapmƒ±yon 2 kurallar √ß...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id      channelTitle  \\\n",
       "758   A7luS6IioBk     Roplex Gaming   \n",
       "4757  vvnjeMtrZvY     Yousef Gaming   \n",
       "1954  o3D9g0ohRIo    Gaming Insects   \n",
       "4553  uWpup9oq54w  Stealth17 Gaming   \n",
       "1021  -iojQuqlWkU         DK Gaming   \n",
       "\n",
       "                                                  title  \\\n",
       "758   Trading MEGA NAGA DRAGON (HUGE OVER PAY ADOPT ME)   \n",
       "4757                   12 ŸÇŸÅŸÑ #6 | ÿ¨ÿ≤ÿ° ÿ¨ÿØŸäÿØ ÿÆÿ±ÿßŸÅŸä üòªüî• !!   \n",
       "1954  üòÇI Became A Giant Alex To Fooled This Girl in ...   \n",
       "4553  The German Fleet Arrives - Ultimate Admiral Dr...   \n",
       "1021  ƒ∞LK √áOCUKLAR ! (THE SIMS 4 100 BEBEK CHALLENGE...   \n",
       "\n",
       "                                            description  \\\n",
       "758   ----------------------------------------------...   \n",
       "4757  ŸÑÿß ÿ™ÿ¥ÿ∫ŸÑŸÉ ÿßŸÑŸÖŸÇÿßÿ∑ÿπ ÿπŸÜ ÿßŸÑÿµŸÑŸàÿßÿ™ üíñ\\nÿµŸÑŸàÿß ÿπŸÑŸâ ŸÖÿ≠ŸÖÿØ Ô∑∫...   \n",
       "1954  üòÇI Became A Giant Alex To Fooled This Girl in ...   \n",
       "4553  The German fleet suddenly showed up in the Asi...   \n",
       "1021  ‚úÖ DK Gaming ten selamlar ben Duygu K√∂seoƒülu bu...   \n",
       "\n",
       "                                                   tags  \\\n",
       "758   ['roblox', 'roplex gaming', 'adopt me', 'roblo...   \n",
       "4757  ['ŸäŸàÿ≥ŸÅ ÿßÿ≠ŸÖÿØ', 'ŸäŸàÿ≥ŸÅ', 'ÿßÿ≠ŸÖÿØ', 'ŸäŸàÿ≥ŸÅ ÿ¨ŸäŸÖŸÜÿ¨', 'y...   \n",
       "1954  ['Trolling My New Girlfriend as GIANT ALEX in ...   \n",
       "4553  ['ultimate admiral dreadnoughts', 'ultimate ad...   \n",
       "1021  ['dkgaming', 'duygu', 'duygu k√∂seoƒülu', '√ºcret...   \n",
       "\n",
       "                    publishedAt viewCount likeCount commentCount definition  \\\n",
       "758   2023-03-03 21:06:32+00:00     61007      1178          281         hd   \n",
       "4757  2022-07-16 14:00:53+00:00     56921      4607          177         hd   \n",
       "1954  2023-03-02 07:30:08+00:00    144703      5977          628         hd   \n",
       "4553  2023-02-20 16:00:36+00:00     13260       691           55         hd   \n",
       "1021  2022-12-05 12:00:11+00:00     30649      1335          189         hd   \n",
       "\n",
       "     duration_in_minutes subscribers total_views                 date  \\\n",
       "758                 9.75      650000   189972458  2019-05-11 00:00:00   \n",
       "4757           12.101562     1790000    95599249  2016-11-05 00:00:00   \n",
       "1954             20.1875      694000    79491185  2019-08-22 00:00:00   \n",
       "4553            45.15625      102000    41969216  2014-08-18 00:00:00   \n",
       "1021             40.3125      252000    54565589  2019-03-07 00:00:00   \n",
       "\n",
       "     video_count                                              about country  \\\n",
       "758          929  I like to do adopt me trading videos on roblox...      US   \n",
       "4757         270  ÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ ÿ≠ÿ®ÿßŸäÿ®Ÿä ŸÅŸä ŸÇŸÜÿßÿ© ÿßŸÑÿ£ŸÑÿπÿßÿ®\\nŸáŸÜÿß ŸÅŸä Yous...      SA   \n",
       "1954         415  Guys Im Trying My Best Ki App Logo Ko Bohot En...      IN   \n",
       "4553        3844  Calm & collected commentaries on strategy game...      NL   \n",
       "1021         515  Daha fazla oyun daha fazla eƒülence! Duygu K√∂se...      TR   \n",
       "\n",
       "                                               comments like_counts  \\\n",
       "758                                         slayyyyyyyy           0   \n",
       "4757                                                  ‚ô°           1   \n",
       "1954                                                Yes           1   \n",
       "4553  Wow, that was one big slaughter !!! I can't aw...           4   \n",
       "1021  1 orada age up var neden yapmƒ±yon 2 kurallar √ß...           0   \n",
       "\n",
       "     reply_counts  \n",
       "758             0  \n",
       "4757            0  \n",
       "1954            0  \n",
       "4553            0  \n",
       "1021            1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d71c2d1-bed5-440e-a7c8-4dd52f37eb00",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  <center> **Extract <span style=\"color: red\">categorical</span> features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d86d3-1556-44fc-b2e8-4b85f74ab79f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### *Extract emojies count per text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d5ae23-d9c9-4307-811a-366b6cadfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_emojis_counts: list = []\n",
    "title_emojis_counts: list = []\n",
    "desc_emojis_counts: list = []\n",
    "\n",
    "for comment, title, desc in zip(df[\"comments\"], df[\"title\"], df[\"description\"]):\n",
    "    \n",
    "    comments_emojis_count: int = 0\n",
    "    title_emojis_count: int = 0\n",
    "    desc_emojis_count: int = 0\n",
    "    \n",
    "    for comment_char, title_char, desc_char in zip(comment, title, desc):\n",
    "        \n",
    "        if emoji.is_emoji(comment_char):\n",
    "            comments_emojis_count += 1\n",
    "            \n",
    "        if emoji.is_emoji(title_char):\n",
    "            title_emojis_count += 1\n",
    "            \n",
    "        if emoji.is_emoji(desc_char):\n",
    "            desc_emojis_count += 1\n",
    "    \n",
    "    comments_emojis_counts.append(comments_emojis_count)\n",
    "    title_emojis_counts.append(title_emojis_count)\n",
    "    desc_emojis_counts.append(desc_emojis_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba335f72-df88-4bc3-ace7-27a991b83a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelTitle</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>definition</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>video_count</th>\n",
       "      <th>about</th>\n",
       "      <th>country</th>\n",
       "      <th>comments</th>\n",
       "      <th>like_counts</th>\n",
       "      <th>reply_counts</th>\n",
       "      <th>comments_emojis_count</th>\n",
       "      <th>title_emojis_count</th>\n",
       "      <th>desc_emojis_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>__g5jjC-Jf8</td>\n",
       "      <td>Vista Gamer</td>\n",
       "      <td>ROBANDO COCHES de POLIC√çA en GTA 5</td>\n",
       "      <td>En el video de hoy robare coches de polic√≠a en...</td>\n",
       "      <td>['gta 5', 'gta v', 'grand theft auto 5', 'gran...</td>\n",
       "      <td>2023-01-30 20:00:09+00:00</td>\n",
       "      <td>1509153</td>\n",
       "      <td>26013</td>\n",
       "      <td>553</td>\n",
       "      <td>hd</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-10 00:00:00</td>\n",
       "      <td>260</td>\n",
       "      <td>CLICK en SUSCRIBIRSE!!\\n\\nSubo contenido relac...</td>\n",
       "      <td>AR</td>\n",
       "      <td>muy buen contenido pero,hazlo sin vida infinit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>p-7c0M0DtlA</td>\n",
       "      <td>Teches Gaming</td>\n",
       "      <td>CEI MAI BUNI JUCATORI DE VALORANT DIN ROMANIA ...</td>\n",
       "      <td>Pur »ôi simplu suntem prea buni. \\n\\nTwitch Adr...</td>\n",
       "      <td>['adriivonb', 'adri', 'adrii', 'adriana', 'twi...</td>\n",
       "      <td>2023-01-24 17:13:06+00:00</td>\n",
       "      <td>18514</td>\n",
       "      <td>1461</td>\n",
       "      <td>55</td>\n",
       "      <td>hd</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-05-14 00:00:00</td>\n",
       "      <td>66</td>\n",
       "      <td></td>\n",
       "      <td>RO</td>\n",
       "      <td>Mai mult val te rog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4620</th>\n",
       "      <td>MzkZyTV85-g</td>\n",
       "      <td>BUDI01 GAMING</td>\n",
       "      <td>Masuk Global Ketemu Ciwi, Eh! Malahan Jumpa Ku...</td>\n",
       "      <td>Join Membership  : \\nhttps://www.youtube.com/c...</td>\n",
       "      <td>['budi01gaming', 'budi01']</td>\n",
       "      <td>2023-01-21 12:27:04+00:00</td>\n",
       "      <td>1635097</td>\n",
       "      <td>63521</td>\n",
       "      <td>2691</td>\n",
       "      <td>hd</td>\n",
       "      <td>...</td>\n",
       "      <td>2019-07-19 00:00:00</td>\n",
       "      <td>297</td>\n",
       "      <td>Contact us :\\n\\nInstagram : budisembrenget\\nht...</td>\n",
       "      <td>ID</td>\n",
       "      <td>Always Random Your Life Budi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>vvnjeMtrZvY</td>\n",
       "      <td>Yousef Gaming</td>\n",
       "      <td>12 ŸÇŸÅŸÑ #6 | ÿ¨ÿ≤ÿ° ÿ¨ÿØŸäÿØ ÿÆÿ±ÿßŸÅŸä üòªüî• !!</td>\n",
       "      <td>ŸÑÿß ÿ™ÿ¥ÿ∫ŸÑŸÉ ÿßŸÑŸÖŸÇÿßÿ∑ÿπ ÿπŸÜ ÿßŸÑÿµŸÑŸàÿßÿ™ üíñ\\nÿµŸÑŸàÿß ÿπŸÑŸâ ŸÖÿ≠ŸÖÿØ Ô∑∫...</td>\n",
       "      <td>['ŸäŸàÿ≥ŸÅ ÿßÿ≠ŸÖÿØ', 'ŸäŸàÿ≥ŸÅ', 'ÿßÿ≠ŸÖÿØ', 'ŸäŸàÿ≥ŸÅ ÿ¨ŸäŸÖŸÜÿ¨', 'y...</td>\n",
       "      <td>2022-07-16 14:00:53+00:00</td>\n",
       "      <td>56921</td>\n",
       "      <td>4607</td>\n",
       "      <td>177</td>\n",
       "      <td>hd</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-11-05 00:00:00</td>\n",
       "      <td>270</td>\n",
       "      <td>ÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ ÿ≠ÿ®ÿßŸäÿ®Ÿä ŸÅŸä ŸÇŸÜÿßÿ© ÿßŸÑÿ£ŸÑÿπÿßÿ®\\nŸáŸÜÿß ŸÅŸä Yous...</td>\n",
       "      <td>SA</td>\n",
       "      <td>ŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸá ÿßŸÑÿπÿ®ÿ© ŸÖÿµÿµŸÖŸÖŸá ÿπ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>xh3Y1Sa7MUg</td>\n",
       "      <td>Gaming Harry</td>\n",
       "      <td>The Medium - Story Explained</td>\n",
       "      <td>Hello everyone, and welcome to the next instal...</td>\n",
       "      <td>['the medium story explained', 'the medium gam...</td>\n",
       "      <td>2023-03-02 15:00:42+00:00</td>\n",
       "      <td>108533</td>\n",
       "      <td>4360</td>\n",
       "      <td>192</td>\n",
       "      <td>hd</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-05-18 00:00:00</td>\n",
       "      <td>464</td>\n",
       "      <td>Hello everyone! Welcome to my channel! My name...</td>\n",
       "      <td>GB</td>\n",
       "      <td>Let‚Äôs face it, most of us watching never playe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id   channelTitle  \\\n",
       "4076  __g5jjC-Jf8    Vista Gamer   \n",
       "3325  p-7c0M0DtlA  Teches Gaming   \n",
       "4620  MzkZyTV85-g  BUDI01 GAMING   \n",
       "4747  vvnjeMtrZvY  Yousef Gaming   \n",
       "827   xh3Y1Sa7MUg   Gaming Harry   \n",
       "\n",
       "                                                  title  \\\n",
       "4076                 ROBANDO COCHES de POLIC√çA en GTA 5   \n",
       "3325  CEI MAI BUNI JUCATORI DE VALORANT DIN ROMANIA ...   \n",
       "4620  Masuk Global Ketemu Ciwi, Eh! Malahan Jumpa Ku...   \n",
       "4747                   12 ŸÇŸÅŸÑ #6 | ÿ¨ÿ≤ÿ° ÿ¨ÿØŸäÿØ ÿÆÿ±ÿßŸÅŸä üòªüî• !!   \n",
       "827                        The Medium - Story Explained   \n",
       "\n",
       "                                            description  \\\n",
       "4076  En el video de hoy robare coches de polic√≠a en...   \n",
       "3325  Pur »ôi simplu suntem prea buni. \\n\\nTwitch Adr...   \n",
       "4620  Join Membership  : \\nhttps://www.youtube.com/c...   \n",
       "4747  ŸÑÿß ÿ™ÿ¥ÿ∫ŸÑŸÉ ÿßŸÑŸÖŸÇÿßÿ∑ÿπ ÿπŸÜ ÿßŸÑÿµŸÑŸàÿßÿ™ üíñ\\nÿµŸÑŸàÿß ÿπŸÑŸâ ŸÖÿ≠ŸÖÿØ Ô∑∫...   \n",
       "827   Hello everyone, and welcome to the next instal...   \n",
       "\n",
       "                                                   tags  \\\n",
       "4076  ['gta 5', 'gta v', 'grand theft auto 5', 'gran...   \n",
       "3325  ['adriivonb', 'adri', 'adrii', 'adriana', 'twi...   \n",
       "4620                         ['budi01gaming', 'budi01']   \n",
       "4747  ['ŸäŸàÿ≥ŸÅ ÿßÿ≠ŸÖÿØ', 'ŸäŸàÿ≥ŸÅ', 'ÿßÿ≠ŸÖÿØ', 'ŸäŸàÿ≥ŸÅ ÿ¨ŸäŸÖŸÜÿ¨', 'y...   \n",
       "827   ['the medium story explained', 'the medium gam...   \n",
       "\n",
       "                    publishedAt viewCount likeCount commentCount definition  \\\n",
       "4076  2023-01-30 20:00:09+00:00   1509153     26013          553         hd   \n",
       "3325  2023-01-24 17:13:06+00:00     18514      1461           55         hd   \n",
       "4620  2023-01-21 12:27:04+00:00   1635097     63521         2691         hd   \n",
       "4747  2022-07-16 14:00:53+00:00     56921      4607          177         hd   \n",
       "827   2023-03-02 15:00:42+00:00    108533      4360          192         hd   \n",
       "\n",
       "      ...                 date video_count  \\\n",
       "4076  ...  2020-02-10 00:00:00         260   \n",
       "3325  ...  2019-05-14 00:00:00          66   \n",
       "4620  ...  2019-07-19 00:00:00         297   \n",
       "4747  ...  2016-11-05 00:00:00         270   \n",
       "827   ...  2020-05-18 00:00:00         464   \n",
       "\n",
       "                                                  about country  \\\n",
       "4076  CLICK en SUSCRIBIRSE!!\\n\\nSubo contenido relac...      AR   \n",
       "3325                                                         RO   \n",
       "4620  Contact us :\\n\\nInstagram : budisembrenget\\nht...      ID   \n",
       "4747  ÿ≥ŸÑÿßŸÖ ÿπŸÑŸäŸÉŸÖ ÿ≠ÿ®ÿßŸäÿ®Ÿä ŸÅŸä ŸÇŸÜÿßÿ© ÿßŸÑÿ£ŸÑÿπÿßÿ®\\nŸáŸÜÿß ŸÅŸä Yous...      SA   \n",
       "827   Hello everyone! Welcome to my channel! My name...      GB   \n",
       "\n",
       "                                               comments like_counts  \\\n",
       "4076  muy buen contenido pero,hazlo sin vida infinit...           1   \n",
       "3325                                Mai mult val te rog           0   \n",
       "4620                       Always Random Your Life Budi           0   \n",
       "4747  ŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸáŸá ÿßŸÑÿπÿ®ÿ© ŸÖÿµÿµŸÖŸÖŸá ÿπ...           0   \n",
       "827   Let‚Äôs face it, most of us watching never playe...           0   \n",
       "\n",
       "     reply_counts comments_emojis_count title_emojis_count desc_emojis_count  \n",
       "4076            0                     0                  0                 0  \n",
       "3325            0                     0                  0                 0  \n",
       "4620            0                     0                  0                 0  \n",
       "4747            0                     0                  2                 1  \n",
       "827             0                     0                  0                 0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments_emojis_count\"] = comments_emojis_counts\n",
    "df[\"title_emojis_count\"] = title_emojis_counts\n",
    "df[\"desc_emojis_count\"] = desc_emojis_counts\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d487f5b-5b6f-4b93-8fd3-0079446e4a74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### *Language detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3385b1-79ec-42f1-9a2b-de5b3801a91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "language_detector = transformers.pipeline(\"text-classification\",\n",
    "                                          model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "                                          use_auth_token= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcab35d-9a81-487f-87dd-962631a5109b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "print(language_detector(\"Ÿáÿ∞ÿß ÿ£ÿ∑ŸàŸÑ ŸÜÿµ ŸÇÿØ ÿ™ŸÇÿ±ÿ£Ÿá ÿπŸÑŸâ ÿßŸÑÿ•ÿ∑ŸÑÿßŸÇ ÿ•ŸÜŸá ÿ∑ŸàŸäŸÑ ŸÑŸÑÿ∫ÿßŸäÿ©\", max_length=128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e40b4-01c6-478f-8672-219a5cb73e0a",
   "metadata": {},
   "source": [
    "Looks like we will take a lot to <ins>just classify the language</ins> of each text at least: $30\\times100$ second whitch equalls <span style=\"color: red\">**50 minutes.**</span><br>\n",
    "So we can't do translation for now becuase it at least may take **5 hours** to run so we will just use this language detetion and<br>\n",
    "optimize it by just sellecting one text column to detect which will be **Description column** and use the detected languages to<br>\n",
    "pridect video and comments language.<br><br>\n",
    "But we will detect the language using the **country of this video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae48b629-95cc-437f-8bfc-87095cfd1382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GB', 'IN', 'TH', 'US', 'TW', 'CA', 'PH', 'SG', 'PL', 'PK', 'DE',\n",
       "       'AU', 'BD', 'ID', 'EG', 'AR', 'TR', 'JP', 'VN', 'NP', 'KR', 'RU',\n",
       "       'BG', 'IT', 'SA', 'MT', 'RO', 'BR', 'NL', 'SE', 'DZ', 'FR', 'FI',\n",
       "       'MX', 'AE', 'HU'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e744162-5f9c-42e1-b6c2-09fd07d179d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country_languages = {\n",
    "    'DE': 'German',\n",
    "    'US': 'English', 'PL': 'Polish',\n",
    "    'SA': 'Arabic', 'NP': 'Nepali',\n",
    "    'CA': 'English', 'ES': 'Spanish',\n",
    "    'TR': 'Turkish', 'IN': 'Hindi',\n",
    "    'EG': 'Arabic', 'GB': 'English',\n",
    "    'MX': 'Spanish', 'BR': 'Portuguese',\n",
    "    'PK': 'Urdu', 'FR': 'French',\n",
    "    'VN': 'Vietnamese', 'ID': 'Indonesian',\n",
    "    'AU': 'English', 'HU': 'Hungarian',\n",
    "    'NL': 'Dutch', 'BG': 'Bulgarian',\n",
    "    'JP': 'Japanese', 'SG': 'English', \n",
    "    'TH': 'Thai', 'PH': 'Tagalog',\n",
    "    'MT': 'Maltese', 'PE': 'Spanish',\n",
    "    'SE': 'Swedish', 'IT': 'Italian',\n",
    "    'KR': 'Korean', 'TW': 'Chinese',\n",
    "    'FI': 'Finnish', 'DZ': 'Arabic',\n",
    "    'BD': 'Bengali', 'AR': 'Spanish'}\n",
    "\n",
    "df[\"language\"] = df[\"country\"].replace(country_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f846902-adaa-4678-b296-f49c5459e4a4",
   "metadata": {},
   "source": [
    "This way may not be so accurate in detecting languages becuase there are some **indian videos** specifically with english and<br>\n",
    "because some commetns come in deffrint languages but at least this way is more accurate than `roberta-language-detection`<br>\n",
    "model because this model sometimes come with wierd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090791f-b041-44a6-90a6-75f463aec37b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### *Sentiment analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840cb75c-2375-45d3-b354-fdfb69ec45ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840c19cc6d3246748d5f3b0e25a6629d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POS', 'score': 0.9883897304534912}]\n"
     ]
    }
   ],
   "source": [
    "sentiment_classifier = transformers.pipeline(model= \"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "print(sentiment_classifier(\"Hello here in my analysis, Have a nice day !!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dcc8fbc-0843-4077-a6b4-d6c83abc7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sentiment_classifier(text: str) -> np.int8:\n",
    "    \"\"\"This code takes a text and return you if it's positive\n",
    "        or negative as 1 for positive, 0 for natural -1 for negative\n",
    "        and -10 for unclassified.\n",
    "    \n",
    "    @params: a string text\n",
    "    @return: 1, 0 or -1 in np.int8 dtype\"\"\"\n",
    "    \n",
    "    sentiment_type_encoder: dict = {\"POS\": 1,\n",
    "                                    \"NEG\": -1,\n",
    "                                    \"NEU\": 0,\n",
    "                                    \"unclassified\": -10}\n",
    "    \n",
    "    try:\n",
    "        sentiment_type: str = sentiment_classifier(text)[0][\"label\"]\n",
    "        \n",
    "    except:\n",
    "        sentiment_type: str = \"unclassified\"\n",
    "        \n",
    "    return  np.int8(sentiment_type_encoder[sentiment_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8916528-4171-4ede-9e99-ff4b3b9ae9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"sentiments\"] = df[\"Comments\"][:5].apply(\n",
    "    lambda x: my_sentiment_classifier(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056e4ad-fc7e-42bc-9bb1-dca16fed8e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"sentiments\", \"Comments\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4b794-24d8-4c52-8ef9-7c2078fca8eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### *Video stats range*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826ebf3-aea5-4381-9097-814d02cad0fa",
   "metadata": {},
   "source": [
    "We will classify each column of **[Comments, Likes, Views]** into 6 groups manually like:\n",
    "<ul>\n",
    "    <li>  $1\\sim3,000$ Views\n",
    "    <li>  $3,000\\sim10,000$ Views\n",
    "    <li>  $10,000\\sim50,000$ Views\n",
    "    <li>  $50,000\\sim100,000$ Views\n",
    "    <li>  $100,000\\sim300,000$ Views\n",
    "    <li>  $300,000\\sim\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5313b-6d7d-4713-bbeb-46b461daceb7",
   "metadata": {},
   "source": [
    "We will see the max calues of each column so we can optimize it with the best dtype for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa25cea-a4d4-4e1f-b5fe-50bf1cc9e6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The max value of view count column is: 67652856\n",
      " The max value of like count column ias: 1164572\n",
      " The max value of comment count column is: 37636\n",
      " The max value of subscribers column is: 32000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\" The max value of view count column is: {df['viewCount'].astype(int).max()}\\n\",\n",
    "      f\"The max value of like count column ias: {df['likeCount'].astype(int).max()}\\n\",\n",
    "      f\"The max value of comment count column is: {df['commentCount'].astype(int).max()}\\n\",\n",
    "      f\"The max value of subscribers column is: {df['subscribers'].astype(int).max()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64e1b26-9182-4b33-8c07-dadafbdb70cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>viewCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>comments_emojis_count</th>\n",
       "      <th>title_emojis_count</th>\n",
       "      <th>desc_emojis_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.078000e+03</td>\n",
       "      <td>6078.000000</td>\n",
       "      <td>6078.000000</td>\n",
       "      <td>6.078000e+03</td>\n",
       "      <td>6078.000000</td>\n",
       "      <td>6078.000000</td>\n",
       "      <td>6078.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.060396e+05</td>\n",
       "      <td>9996.761270</td>\n",
       "      <td>796.926621</td>\n",
       "      <td>1.552624e+06</td>\n",
       "      <td>0.668476</td>\n",
       "      <td>0.161731</td>\n",
       "      <td>0.125041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.939468e+06</td>\n",
       "      <td>13534.257747</td>\n",
       "      <td>2641.105774</td>\n",
       "      <td>3.037447e+06</td>\n",
       "      <td>2.880683</td>\n",
       "      <td>0.495166</td>\n",
       "      <td>0.411487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.186000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>6.720000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.338550e+04</td>\n",
       "      <td>1517.250000</td>\n",
       "      <td>128.250000</td>\n",
       "      <td>1.980000e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.777500e+04</td>\n",
       "      <td>4016.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>4.985000e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.112778e+05</td>\n",
       "      <td>11301.000000</td>\n",
       "      <td>628.000000</td>\n",
       "      <td>1.690000e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.765286e+07</td>\n",
       "      <td>63521.000000</td>\n",
       "      <td>37636.000000</td>\n",
       "      <td>3.200000e+07</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          viewCount     likeCount  commentCount   subscribers  \\\n",
       "count  6.078000e+03   6078.000000   6078.000000  6.078000e+03   \n",
       "mean   6.060396e+05   9996.761270    796.926621  1.552624e+06   \n",
       "std    3.939468e+06  13534.257747   2641.105774  3.037447e+06   \n",
       "min    2.186000e+03      1.000000     51.000000  6.720000e+03   \n",
       "25%    3.338550e+04   1517.250000    128.250000  1.980000e+05   \n",
       "50%    9.777500e+04   4016.000000    257.000000  4.985000e+05   \n",
       "75%    3.112778e+05  11301.000000    628.000000  1.690000e+06   \n",
       "max    6.765286e+07  63521.000000  37636.000000  3.200000e+07   \n",
       "\n",
       "       comments_emojis_count  title_emojis_count  desc_emojis_count  \n",
       "count            6078.000000         6078.000000        6078.000000  \n",
       "mean                0.668476            0.161731           0.125041  \n",
       "std                 2.880683            0.495166           0.411487  \n",
       "min                 0.000000            0.000000           0.000000  \n",
       "25%                 0.000000            0.000000           0.000000  \n",
       "50%                 0.000000            0.000000           0.000000  \n",
       "75%                 0.000000            0.000000           0.000000  \n",
       "max                64.000000            5.000000           4.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.astype({\"commentCount\": np.uint16, \"viewCount\": np.uint32,\n",
    "                \"likeCount\": np.uint16, \"subscribers\": np.uint32})\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6bd3f47-9eae-4d3d-a60d-f7a74b3ff44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"cat_view_count\"] = pd.cut(df['viewCount'],\n",
    "                         bins=[1, 3_000, 10_000, 50_000, 100_000, 300_000, 999_999_999_999],\n",
    "                         labels=[\"from 1 to 3,000\", \"from 3,000 to 10,000\",\n",
    "                                 \"from 10,000 to 50,000\", \"from 50,000 to 100,000\",\n",
    "                                 \"from 100,000 to 300,000\", \"more than 300,000\"])\n",
    "\n",
    "df[\"cat_comment_count\"] = pd.cut(df['commentCount'],\n",
    "                         bins=[50, 100, 150, 200, 400, 600, 999_999_999_999],\n",
    "                         labels=[\"from 50 to 100\", \"from 100 to 150\",\n",
    "                                 \"from 150 to 200\", \"from 200 to 400\",\n",
    "                                 \"from 400 to 600\", \"more than 600\"])\n",
    "\n",
    "df[\"cat_like_count\"] = pd.cut(df['likeCount'],\n",
    "                         bins=[1, 1_000, 5_000, 10_000, 50_000, 150_000, 999_999_999_999],\n",
    "                         labels=[\"from 1 to 1,000\", \"from 1,000 to 5,000\",\n",
    "                                 \"from 5,000 to 10,000\", \"from 10,000 to 50,000\",\n",
    "                                 \"from 50,000 to 150,000\", \"more than 150,000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d15043a-4b10-4ba3-a74b-c05fa3b15fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_view_count</th>\n",
       "      <th>cat_comment_count</th>\n",
       "      <th>cat_like_count</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>likeCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>more than 300,000</td>\n",
       "      <td>more than 600</td>\n",
       "      <td>from 10,000 to 50,000</td>\n",
       "      <td>772444</td>\n",
       "      <td>906</td>\n",
       "      <td>38741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>from 10,000 to 50,000</td>\n",
       "      <td>from 50 to 100</td>\n",
       "      <td>from 1,000 to 5,000</td>\n",
       "      <td>22361</td>\n",
       "      <td>61</td>\n",
       "      <td>3720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>from 100,000 to 300,000</td>\n",
       "      <td>from 50 to 100</td>\n",
       "      <td>from 1,000 to 5,000</td>\n",
       "      <td>154774</td>\n",
       "      <td>89</td>\n",
       "      <td>3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>from 50,000 to 100,000</td>\n",
       "      <td>from 400 to 600</td>\n",
       "      <td>from 1,000 to 5,000</td>\n",
       "      <td>86072</td>\n",
       "      <td>487</td>\n",
       "      <td>3136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>from 100,000 to 300,000</td>\n",
       "      <td>more than 600</td>\n",
       "      <td>from 10,000 to 50,000</td>\n",
       "      <td>263801</td>\n",
       "      <td>952</td>\n",
       "      <td>26395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               cat_view_count cat_comment_count         cat_like_count  \\\n",
       "3775        more than 300,000     more than 600  from 10,000 to 50,000   \n",
       "3132    from 10,000 to 50,000    from 50 to 100    from 1,000 to 5,000   \n",
       "5057  from 100,000 to 300,000    from 50 to 100    from 1,000 to 5,000   \n",
       "5015   from 50,000 to 100,000   from 400 to 600    from 1,000 to 5,000   \n",
       "2236  from 100,000 to 300,000     more than 600  from 10,000 to 50,000   \n",
       "\n",
       "      viewCount  commentCount  likeCount  \n",
       "3775     772444           906      38741  \n",
       "3132      22361            61       3720  \n",
       "5057     154774            89       3850  \n",
       "5015      86072           487       3136  \n",
       "2236     263801           952      26395  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"cat_view_count\", \"cat_comment_count\", \"cat_like_count\",\n",
    "    \"viewCount\", \"commentCount\", \"likeCount\"]].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f83d0d-3080-4ce4-95a9-2035bad6634a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### *Text columns length*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f34f55aa-01c0-431b-9451-cfc1b4515e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in TEXT_COLUMNS:\n",
    "    df[f\"{col}_length\"] = df[col].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81007d35-99d7-4b33-b066-e58fda430082",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <center><strong>Text visualiztion with<span style = \"color: red\"> WordCloud</span></strong></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb11cf8d-2669-4851-85ab-51e7bdfb3827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coloring = np.array(Image.open(\"../imgs/youtube_gaming_logo.png\"))\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "comments = ''.join(df[\"comments\"][df[\"language\"] == \"English\"].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "135ea87b-5990-47ee-9840-358f2ca2b06c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m comments_wc \u001b[38;5;241m=\u001b[39m WordCloud(background_color\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m, mask\u001b[38;5;241m=\u001b[39m coloring,\n\u001b[0;32m      2\u001b[0m                stopwords\u001b[38;5;241m=\u001b[39m stopwords, max_font_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m      3\u001b[0m                font_path\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data analysis/assets/fonts/FranklinGothic.ttf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m                collocations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m image_colors \u001b[38;5;241m=\u001b[39m ImageColorGenerator(np\u001b[38;5;241m.\u001b[39marray(coloring))\n\u001b[1;32m----> 7\u001b[0m \u001b[43mcomments_wc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m comments_wc \u001b[38;5;241m=\u001b[39m comments_wc\u001b[38;5;241m.\u001b[39mrecolor(color_func\u001b[38;5;241m=\u001b[39m image_colors)\n\u001b[0;32m     10\u001b[0m comments_svg \u001b[38;5;241m=\u001b[39m comments_wc\u001b[38;5;241m.\u001b[39mto_svg(embed_font\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\wordcloud\\wordcloud.py:639\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\wordcloud\\wordcloud.py:621\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    620\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 621\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\wordcloud\\wordcloud.py:410\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    408\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(frequencies\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frequencies) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe need at least 1 word to plot a word cloud, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(frequencies))\n\u001b[0;32m    412\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m frequencies[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_words]\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# largest entry will be 1\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "comments_wc = WordCloud(background_color= \"white\", max_words= 300, mask= coloring,\n",
    "               stopwords= stopwords, max_font_size= 80, random_state= 42,\n",
    "               font_path= '../Data analysis/assets/fonts/FranklinGothic.ttf',\n",
    "               collocations=False)\n",
    "\n",
    "image_colors = ImageColorGenerator(np.array(coloring))\n",
    "comments_wc.generate(comments)\n",
    "comments_wc = comments_wc.recolor(color_func= image_colors)\n",
    "\n",
    "comments_svg = comments_wc.to_svg(embed_font=True)\n",
    "\n",
    "# Save the SVG code to a file\n",
    "with open(\"../plots/comments_word_cloud.svg\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(comments_svg)\n",
    "    \n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.imshow(comments_wc, interpolation=\"bilinear\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.title(\"Comments word cloud\")\n",
    "fig.set_size_inches(10, 8)\n",
    "plt.savefig(\"../plots/comments_word_cloud.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d205eae-0082-403d-85a3-e7eff934769b",
   "metadata": {},
   "source": [
    "Now we will make a word cloud for **video titles data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9cf85c9-fa11-4bf6-af23-18962eae6313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "popular_emojis = [\"üòä\", \"üòç\", \"üòò\", \"üòú\", \"üòù\", \"üòÇ\", \"üò≠\", \"üò°\", \"üò†\", \"üò©\", \"üò´\", \"üòû\", \"üòü\", \"üò¢\", \"üò•\", \"üò∞\", \"üò±\",\n",
    "                       \"üò≥\", \"üò∑\", \"üëç\", \"üëé\", \"üëå\", \"üëè\", \"üôå\", \"üëã\", \"üí™\", \"üôè\", \"‚ù§Ô∏è\", \"üíî\", \"üíï\", \"üíñ\", \"üíò\", \"üíô\", \"üíö\",\n",
    "                       \"üíõ\", \"üíú\", \"üíØ\", \"üî•\", \"üåü\", \"‚ú®\", \"‚≠ê\", \"üåà\", \"üå∫\", \"üçï\", \"üçî\", \"üçü\", \"üç¶\", \"üç≠\", \"üç©\", \"üç™\", \"üç∫\",\n",
    "                       \"üçª\", \"üç∑\", \"üç∏\", \"üéÇ\", \"üéÅ\", \"üéâ\", \"üéä\", \"üéà\", \"üéµ\", \"üé∂\", \"üéº\", \"üéß\", \"üé§\", \"üé∏\", \"üéπ\", \"üé∫\", \"üé∑\",\n",
    "                       \"üéª\", \"üé¨\", \"üé•\", \"üé¶\", \"üì∑\", \"üìπ\", \"üì∫\", \"üìª\", \"üíª\", \"üì±\", \"üí°\", \"üîë\", \"üî®\", \"üî•\", \"üí∞\", \"üí≥\", \"üíº\",\n",
    "                       \"üìÖ\", \"üìÜ\", \"üìà\", \"üìâ\", \"üìä\", \"üìã\", \"üìé\", \"üìè\", \"üìê\", \"üîí\", \"üîì\", \"üîç\", \"üîé\", \"üöÄ\", \"üöë\", \"üöí\", \"üöì\",\n",
    "                       \"üöï\", \"üöó\", \"üöô\", \"üöö\", \"üö¢\", \"üö§\", \"üö≤\", \"üö∂\", \"üö∂‚Äç‚ôÄÔ∏è\", \"üèÉ\", \"üèÉ‚Äç‚ôÄÔ∏è\", \"‚öΩ\", \"üèÄ\", \"üèà\", \"üéæ\", \"üèê\", \"üèâ\",\n",
    "                       \"üé±\", \"üèì\", \"üè∏\", \"ü•ä\", \"ü•ã\", \"üéÆ\", \"üïπÔ∏è\", \"üé≤\", \"üÉè\"]\n",
    "\n",
    "\n",
    "for emoji in popular_emojis:\n",
    "    stopwords.add(emoji)\n",
    "    \n",
    "titels = ''.join(df[\"title\"][df[\"language\"] == \"English\"].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2ef628a-ad55-4443-b30b-d4ba0a0c777e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m coloring \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../imgs/joystick.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m titels_wc \u001b[38;5;241m=\u001b[39m WordCloud(background_color\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_words\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m, mask\u001b[38;5;241m=\u001b[39m coloring,\n\u001b[0;32m      4\u001b[0m                stopwords\u001b[38;5;241m=\u001b[39m stopwords, max_font_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m      5\u001b[0m                font_path\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data analysis/assets/fonts/FranklinGothic.ttf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtitels_wc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m image_colors \u001b[38;5;241m=\u001b[39m ImageColorGenerator(np\u001b[38;5;241m.\u001b[39marray(coloring))\n\u001b[0;32m      9\u001b[0m titels_wc \u001b[38;5;241m=\u001b[39m titels_wc\u001b[38;5;241m.\u001b[39mrecolor(color_func\u001b[38;5;241m=\u001b[39m image_colors)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\wordcloud\\wordcloud.py:639\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\wordcloud\\wordcloud.py:621\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    620\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 621\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\wordcloud\\wordcloud.py:410\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    408\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(frequencies\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frequencies) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe need at least 1 word to plot a word cloud, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(frequencies))\n\u001b[0;32m    412\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m frequencies[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_words]\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# largest entry will be 1\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "coloring = np.array(Image.open(\"../imgs/joystick.png\"))\n",
    "\n",
    "titels_wc = WordCloud(background_color= \"white\", max_words= 2000, mask= coloring,\n",
    "               stopwords= stopwords, max_font_size= 80, random_state= 42,\n",
    "               font_path= '../Data analysis/assets/fonts/FranklinGothic.ttf')\n",
    "\n",
    "titels_wc.generate(titels)\n",
    "image_colors = ImageColorGenerator(np.array(coloring))\n",
    "titels_wc = titels_wc.recolor(color_func= image_colors)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.imshow(titels_wc, interpolation= \"bilinear\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "titels_svg = titels_wc.to_svg(embed_font=True)\n",
    "\n",
    "with open(\"../plots/videos_titels_word_cloud.svg\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(titels_svg)\n",
    "\n",
    "plt.title(\"Video titles word cloud\")\n",
    "fig.set_size_inches(10, 8)\n",
    "plt.savefig(\"../plots/videos_titels_word_cloud.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5551db7-f409-4c22-9a63-60b297f2f104",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style = \"color: red\">NLP</span> preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b30e29d-2854-4429-bfc8-e20d6b7681ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Removing stop words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c99fa01-d3b2-4484-bc19-6b53499403c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Error loading stopwords-hi: Package 'stopwords-hi' not\n",
      "[nltk_data]     found in index\n",
      "[nltk_data] Error loading stopwords-ar: Package 'stopwords-ar' not\n",
      "[nltk_data]     found in index\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\FreeComp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download()\n",
    "\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('wordnet', force= True)\n",
    "nltk.download('stopwords', force=True)\n",
    "nltk.download('stopwords-hi', force=True)\n",
    "nltk.download('stopwords-ar', force=True)\n",
    "nltk.download('averaged_perceptron_tagger', force=True)\n",
    "\n",
    "\n",
    "en_stopwords = set(stopwords.words('english')) \n",
    "ar_stopwords = set(stopwords.words('arabic')) \n",
    "# hi_stopwords = set(stopwords.words('hindi')) \n",
    "\n",
    "all_stopwords = en_stopwords.union(ar_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae43419a-e185-45c0-8a2a-6a308eca0989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:7: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\FreeComp\\AppData\\Local\\Temp\\ipykernel_8952\\3746631438.py:7: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  filtered_words = list(filter(lambda item: item is not \"\", filtered_words))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [msq, suffer, irreparable, emotional, damage, ...\n",
       "1       [even, joke, description, elezen, forgets, dus...\n",
       "2                                                  [cool]\n",
       "3       [damn, d, love, able, enjoy, ff14, universe, r...\n",
       "4       [emotional, trauma, way, floorior, truer, word...\n",
       "                              ...                        \n",
       "6073    [ŸäŸàÿ≥ŸÅ, ÿßŸÑÿ≠ŸÑŸÇŸá, ÿßŸÑÿ¨ÿßŸäŸá, ÿ±Ÿàÿ≠, ÿßŸÑŸÉŸáŸÅ, ÿßÿ¥ÿ™ÿ∫ŸÑ, ÿ®ÿ™ŸÑŸÇ...\n",
       "6074                                         [ŸäŸàÿ≥ŸÅ, ÿ®ÿ≠ÿ®ŸÉ]\n",
       "6075    [ŸäŸàÿ≥ŸÅ, ÿßÿ∞ÿß, ÿ≥ŸàŸäÿ´, ŸÖŸáŸÖÿ©, ÿ™ŸÑÿ≥ŸÉŸàÿ®, ŸÅŸÑÿßÿ≤ŸÖ, ÿ™ÿ±Ÿàÿ≠, ŸÑ...\n",
       "6076    [ŸäŸàÿ≥ŸÅ, ÿßŸÑÿπÿ®, dying, light, 2, ÿßŸÑŸÑÿπÿ®ÿ©, ŸÉÿ´Ÿäÿ±, ÿ≠ŸÑ...\n",
       "6077    [ŸäŸàÿ≥ŸÅ, ÿ¥ŸÉÿ±ÿß, Ÿáÿßÿ∞Ÿä, ÿßŸÑŸÅŸäÿØŸäŸàŸáÿßÿ™, ÿßŸÑÿ¨ŸÖŸäŸÑŸá, ÿßÿ™ŸÖŸÜŸâ,...\n",
       "Name: comments_tokens, Length: 6078, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stopwords_dropper(words: list, stopwords: set) -> list:\n",
    "    \n",
    "    # Removing stop words from unalphabetical chars\n",
    "    filtered_words = [re.sub(r\"[\\W_]\", \"\", word) for word in words\n",
    "                      if not word in stopwords]\n",
    "    \n",
    "    filtered_words = list(filter(lambda item: item is not \"\", filtered_words))\n",
    "    return  filtered_words\n",
    "\n",
    "\n",
    "for col in TEXT_COLUMNS:\n",
    "    df[f\"{col}_tokens\"] = df[col].apply(lambda text: nltk.word_tokenize(text.lower()))\n",
    "    df[f\"{col}_tokens\"] = df[f\"{col}_tokens\"].apply(lambda text: stopwords_dropper(text,\n",
    "                                                                     all_stopwords))\n",
    "\n",
    "df[\"comments_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153b801-9d49-457e-b771-8d37fe7d982c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Part-of-speech (POS) Tagging*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7aaeeb6-f0cc-4ddb-9a50-6b6d54169268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(pg, NN), (team, NN), (variety, NN), (gaming,...\n",
       "1       [(pg, NN), (team, NN), (variety, NN), (gaming,...\n",
       "2       [(pg, NN), (team, NN), (variety, NN), (gaming,...\n",
       "3       [(pg, NN), (team, NN), (variety, NN), (gaming,...\n",
       "4       [(pg, NN), (team, NN), (variety, NN), (gaming,...\n",
       "                              ...                        \n",
       "6073    [(ÿ≥ŸÑÿßŸÖ, JJ), (ÿπŸÑŸäŸÉŸÖ, NNP), (ÿ≠ÿ®ÿßŸäÿ®Ÿä, NNP), (ŸÇŸÜÿß...\n",
       "6074    [(ÿ≥ŸÑÿßŸÖ, JJ), (ÿπŸÑŸäŸÉŸÖ, NNP), (ÿ≠ÿ®ÿßŸäÿ®Ÿä, NNP), (ŸÇŸÜÿß...\n",
       "6075    [(ÿ≥ŸÑÿßŸÖ, JJ), (ÿπŸÑŸäŸÉŸÖ, NNP), (ÿ≠ÿ®ÿßŸäÿ®Ÿä, NNP), (ŸÇŸÜÿß...\n",
       "6076    [(ÿ≥ŸÑÿßŸÖ, JJ), (ÿπŸÑŸäŸÉŸÖ, NNP), (ÿ≠ÿ®ÿßŸäÿ®Ÿä, NNP), (ŸÇŸÜÿß...\n",
       "6077    [(ÿ≥ŸÑÿßŸÖ, JJ), (ÿπŸÑŸäŸÉŸÖ, NNP), (ÿ≠ÿ®ÿßŸäÿ®Ÿä, NNP), (ŸÇŸÜÿß...\n",
       "Name: about_pos_tags, Length: 6078, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in TEXT_COLUMNS:\n",
    "    df[f\"{col}_pos_tags\"] = df[f\"{col}_tokens\"].apply(lambda words: nltk.pos_tag(words))\n",
    "\n",
    "df[\"about_pos_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147edc4-d4a1-48c1-9fa7-5ed1242ecec4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *Lemmatization and dropping duplicated words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64504cf0-014a-42ab-aa9b-730cfd69a511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag: str) -> str:\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a77cc152-86d7-4df7-901d-2f223e6e0187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.stem.wordnet' has no attribute 'ADJ'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token, pos_tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m], row[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pos_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m----> 9\u001b[0m         wordnet_pos \u001b[38;5;241m=\u001b[39m \u001b[43mget_wordnet_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m         lemmatized_words_group\u001b[38;5;241m.\u001b[39mappend(lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token, pos\u001b[38;5;241m=\u001b[39m wordnet_pos))\n\u001b[0;32m     11\u001b[0m         lemmatized_words_group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(lemmatized_words_group)) \u001b[38;5;66;03m# Dropping duplicates\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m, in \u001b[0;36mget_wordnet_pos\u001b[1;34m(treebank_tag)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_wordnet_pos\u001b[39m(treebank_tag: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m treebank_tag\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mADJ\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m treebank_tag\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wordnet\u001b[38;5;241m.\u001b[39mVERB\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk.stem.wordnet' has no attribute 'ADJ'"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "lemmatized_words_group = []\n",
    "\n",
    "for col in TEXT_COLUMNS:\n",
    "    for index, row in df.iterrows():\n",
    "        for token, pos_tag in zip(row[f\"{col}_tokens\"], row[f\"{col}_pos_tags\"]):\n",
    "\n",
    "            wordnet_pos = get_wordnet_pos(pos_tag[1])\n",
    "            lemmatized_words_group.append(lemmatizer.lemmatize(token, pos= wordnet_pos))\n",
    "            lemmatized_words_group = list(set(lemmatized_words_group)) # Dropping duplicates\n",
    "\n",
    "\n",
    "        lemmatized_words.append(lemmatized_words_group)\n",
    "        lemmatized_words_group = [] # clearing this list\n",
    "    \n",
    "    df[f\"{col}_tokens\"] = lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debe01c-effc-44ff-b48b-f7907907ce8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"channelTitle_tokens\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fe229-ec6c-41f1-ab24-a2eef29cd9dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### *One hot encoding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae7470c3-dfeb-429f-bdf3-7d124daf1b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"language\"] = df[\"language\"].astype(\"category\").cat.codes\n",
    "df[\"definition\"] = df[\"definition\"].astype(\"category\").cat.codes\n",
    "# df[\"sentiments\"] = df[\"sentiments\"].astype(\"category\")\n",
    "\n",
    "df[\"cat_view_count\"] = df[\"cat_view_count\"].replace({\"from 1 to 3,000\": 1, \"from 3,000 to 10,000\": 2,\n",
    "                                                     \"from 10,000 to 50,000\": 3, \"from 50,000 to 100,000\": 4,\n",
    "                                                     \"from 100,000 to 300,000\": 5, \"more than 300,000\": 6})\n",
    "\n",
    "df[\"cat_like_count\"] = df[\"cat_like_count\"].replace({\"from 1 to 1,000\": 1, \"from 1,000 to 5,000\": 2,\n",
    "                                                     \"from 5,000 to 10,000\": 3, \"from 10,000 to 50,000\": 4,\n",
    "                                                     \"from 50,000 to 150,000\": 5, \"more than 150,000\": 6})\n",
    "\n",
    "df[\"cat_comment_count\"] = df[\"cat_comment_count\"].replace({\"from 50 to 100\": 1, \"from 100 to 150\": 2,\n",
    "                                                           \"from 150 to 200\": 3, \"from 200 to 400\": 4,\n",
    "                                                           \"from 400 to 600\": 5, \"more than 600\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5df1754-3787-4ad3-bdf3-d64d6f3abb7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>definition</th>\n",
       "      <th>cat_view_count</th>\n",
       "      <th>cat_like_count</th>\n",
       "      <th>cat_comment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   language  definition cat_view_count cat_like_count cat_comment_count\n",
       "0         6           0              3              2                 4\n",
       "1         6           0              3              2                 4\n",
       "2         6           0              3              2                 4\n",
       "3         6           0              3              2                 4\n",
       "4         6           0              3              2                 4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"language\", \"definition\", \"cat_view_count\",\n",
    "    \"cat_like_count\", \"cat_comment_count\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb03b0-4af0-49eb-9ca4-48785a22f5dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <center><strong><span style = \"color: red\">Text</span> classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e05670e1-d0a0-4e77-83d9-126ce045b109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>definition</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>about_tokens</th>\n",
       "      <th>description_tokens</th>\n",
       "      <th>comments_tokens</th>\n",
       "      <th>comments_length</th>\n",
       "      <th>description_length</th>\n",
       "      <th>about_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>channelTitle_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[runner, vs, deadly, bus, gta5]</td>\n",
       "      <td>[welcome, get, good, gaming, make, best, lan, ...</td>\n",
       "      <td>[escape, cops, gta5, https, youtubexi6nr3gv5ho...</td>\n",
       "      <td>[life, changed]</td>\n",
       "      <td>18</td>\n",
       "      <td>653</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[8, crazy, things, old, lol, players, remember]</td>\n",
       "      <td>[league, legends, jeremy, gaming, curios, vide...</td>\n",
       "      <td>[league, legends, lol, 8, crazy, things, old, ...</td>\n",
       "      <td>[picked, game, late, s1, fun, look, back, mome...</td>\n",
       "      <td>77</td>\n",
       "      <td>1968</td>\n",
       "      <td>101</td>\n",
       "      <td>49</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2789</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[predator, orion, 7000, upgradeable, liquid, c...</td>\n",
       "      <td>[want, see, ve, cooking, new, gaming, tech, un...</td>\n",
       "      <td>[treat, new, gaming, pc, supreme, taskmasterin...</td>\n",
       "      <td>[bought, 12th, gen, 5000, ve, seen, alarming, ...</td>\n",
       "      <td>686</td>\n",
       "      <td>528</td>\n",
       "      <td>153</td>\n",
       "      <td>57</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>[buying, 1, million, rolls, royce, minecraft]</td>\n",
       "      <td>[pro, players, subscribe, hs, gaming, business...</td>\n",
       "      <td>[vlogging, channel, msvlogs35, little, brother...</td>\n",
       "      <td>[boat]</td>\n",
       "      <td>4</td>\n",
       "      <td>394</td>\n",
       "      <td>107</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[comparing, ranking, every, future, paradox, p...</td>\n",
       "      <td>[false, swipe, gaming, aims, make, quality, vi...</td>\n",
       "      <td>[compare, every, future, paradox, mon, origina...</td>\n",
       "      <td>[love, cause, nt, caught, generation, pokemon]</td>\n",
       "      <td>69</td>\n",
       "      <td>766</td>\n",
       "      <td>159</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      language  definition                                       title_tokens  \\\n",
       "351          6           0                    [runner, vs, deadly, bus, gta5]   \n",
       "3612         6           0    [8, crazy, things, old, lol, players, remember]   \n",
       "2789         6           0  [predator, orion, 7000, upgradeable, liquid, c...   \n",
       "299         27           0      [buying, 1, million, rolls, royce, minecraft]   \n",
       "1504         6           0  [comparing, ranking, every, future, paradox, p...   \n",
       "\n",
       "                                           about_tokens  \\\n",
       "351   [welcome, get, good, gaming, make, best, lan, ...   \n",
       "3612  [league, legends, jeremy, gaming, curios, vide...   \n",
       "2789  [want, see, ve, cooking, new, gaming, tech, un...   \n",
       "299   [pro, players, subscribe, hs, gaming, business...   \n",
       "1504  [false, swipe, gaming, aims, make, quality, vi...   \n",
       "\n",
       "                                     description_tokens  \\\n",
       "351   [escape, cops, gta5, https, youtubexi6nr3gv5ho...   \n",
       "3612  [league, legends, lol, 8, crazy, things, old, ...   \n",
       "2789  [treat, new, gaming, pc, supreme, taskmasterin...   \n",
       "299   [vlogging, channel, msvlogs35, little, brother...   \n",
       "1504  [compare, every, future, paradox, mon, origina...   \n",
       "\n",
       "                                        comments_tokens  comments_length  \\\n",
       "351                                     [life, changed]               18   \n",
       "3612  [picked, game, late, s1, fun, look, back, mome...               77   \n",
       "2789  [bought, 12th, gen, 5000, ve, seen, alarming, ...              686   \n",
       "299                                              [boat]                4   \n",
       "1504     [love, cause, nt, caught, generation, pokemon]               69   \n",
       "\n",
       "      description_length  about_length  title_length  channelTitle_length  \n",
       "351                  653            90            29                   15  \n",
       "3612                1968           101            49                   22  \n",
       "2789                 528           153            57                   15  \n",
       "299                  394           107            45                    9  \n",
       "1504                 766           159            71                   18  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[[\"language\", \"definition\", \"title_tokens\", \"about_tokens\",\n",
    "        \"description_tokens\", \"comments_tokens\", \"comments_length\",\n",
    "        \"description_length\", \"about_length\", \"title_length\",\n",
    "        \"channelTitle_length\"]]\n",
    "\n",
    "y = df[[\"cat_like_count\", \"cat_view_count\", \"cat_comment_count\"]]\n",
    "\n",
    "X.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3541229c-4843-4bd6-b623-66fed630ebb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1/ 5, random_state= 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f998b889-3c59-424c-a2e2-0058aed1dc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_like_count</th>\n",
       "      <th>cat_view_count</th>\n",
       "      <th>cat_comment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4227</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4862 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cat_like_count cat_view_count cat_comment_count\n",
       "191               2              5                 4\n",
       "2955              1              3                 4\n",
       "1127              2              4                 4\n",
       "2160              1              3                 4\n",
       "4227              2              2                 2\n",
       "...             ...            ...               ...\n",
       "1768              2              5                 1\n",
       "1737              2              3                 2\n",
       "3240              3              5                 6\n",
       "5305              1              2                 2\n",
       "4737              3              5                 6\n",
       "\n",
       "[4862 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd48fa5-cfa1-4fa0-b0ae-fb303ed5356a",
   "metadata": {},
   "source": [
    "#### *XGBoost classfier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "293f94e8-7ab4-440d-b746-7b60d50f8882",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31], got [1 2 nan 2 3 nan 3 nan 3 4 nan 4 nan 4 5 nan 5 nan 5 6 nan 6 nan 6 nan 6\n nan 6 nan 6 nan 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m text_clf \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m, TfidfVectorizer()),\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m, XGBClassifier()),\n\u001b[0;32m      4\u001b[0m ])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Fit the model and make predictions\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtext_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m text_clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluate model performance\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\youtube_gaming\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\youtube_gaming\\lib\\site-packages\\xgboost\\sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1435\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1443\u001b[0m     )\n\u001b[0;32m   1445\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31], got [1 2 nan 2 3 nan 3 nan 3 4 nan 4 nan 4 5 nan 5 nan 5 6 nan 6 nan 6 nan 6\n nan 6 nan 6 nan 6]"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', XGBClassifier()),\n",
    "])\n",
    "\n",
    "# Fit the model and make predictions\n",
    "text_clf.fit(X_train, y_train)\n",
    "y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc59898-afbe-44ef-a0ef-f5304a88a241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
